{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74115f2c-5259-4f1d-a759-91b87f5bd9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a range of dayobs values to search - \n",
    "day_obs_min = \"Today\"\n",
    "day_obs_min = \"2024-11-25\"\n",
    "day_obs_max = \"Today\"\n",
    "day_obs_max = \"2024-11-26\"\n",
    "time_order = 'newest first'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ac8228-7002-4ed1-b23f-c1563e5723e2",
   "metadata": {},
   "source": [
    "# EFD Scripts + Logs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d21c3a-158c-4713-b704-b974d1f80644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import option_context\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from astropy.time import Time, TimeDelta\n",
    "import datetime\n",
    "import astropy.units as u\n",
    "import requests\n",
    "import yaml\n",
    "from enum import Enum\n",
    "\n",
    "# To generate a tiny gap in time\n",
    "EPS_TIME = np.timedelta64(1, 'ms')\n",
    "TIMESTAMP_ZERO = Time(0, format='unix_tai').utc.datetime\n",
    "\n",
    "from lsst_efd_client import EfdClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d01e192-a02c-4900-9bfb-5d50e8bb8ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsst-ts-xml is in conda\n",
    "# See https://github.com/lsst-ts/ts_xml/blob/develop/python/lsst/ts/xml/enums \n",
    "from lsst.ts.xml.sal_enums import State as CSCState\n",
    "from lsst.ts.xml.enums.ScriptQueue import ScriptProcessState, SalIndex\n",
    "from lsst.ts.xml.enums.Script import ScriptState\n",
    "from lsst.ts.xml.enums.Watcher import AlarmSeverity\n",
    "\n",
    "from lsst.summit.utils import getSite\n",
    "from lsst.summit.utils.efdUtils import makeEfdClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0abe2-3554-4afd-84cf-e28067008911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run as 'apply' per row (axis=1)\n",
    "def apply_enum(x: pd.Series, column: str, enumvals: Enum) -> str:\n",
    "    return enumvals(x[column]).name\n",
    "\n",
    "\n",
    "def get_clients() -> dict:\n",
    "    \"\"\"Return site-specific client connections. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    endpoints : `dict`\n",
    "        Dictionary with `efd`, `obsenv`, \n",
    "        `narrative_log`, and `exposure_log`\n",
    "        connection information.\n",
    "        For the obsenv, narrative log and exposure log, these are only\n",
    "        defined for the summit or USDF.\n",
    "    \"\"\"\n",
    "    site = getSite()\n",
    "    # This will fail if site is \"unknown\"\n",
    "    # although could go with usdf_efd ..\n",
    "    efd_client = makeEfdClient()\n",
    "    if site == \"summit\":\n",
    "        API_BASE = \"https://summit-lsp.lsst.codes/\"\n",
    "        obsenv_client = EfdClient('summit_efd', db_name='lsst.obsenv')\n",
    "    elif site == \"tucson\" or site == \"base\":\n",
    "        # I don't know what the logging looks like here, \n",
    "        # but suspect it doesn't exist\n",
    "        obsenv_client = None\n",
    "        API_BASE = None\n",
    "    else:\n",
    "        obsenv_client = EfdClient('usdf_efd', db_name='lsst.obsenv')\n",
    "        API_BASE = \"https://usdf-rsp.slac.stanford.edu/\"\n",
    "    narrative_log_url =  API_BASE + \"narrativelog/messages\"\n",
    "    exposure_log_url = API_BASE + \"exposurelog/messages\"\n",
    "    return {'efd': efd_client, 'obsenv': obsenv_client, \n",
    "            'narrative_log': narrative_log_url, 'exposure_log': exposure_log_url}\n",
    "\n",
    "\n",
    "def query_logging_services(API_ENDPOINT: str, params: dict, return_dataframe: bool =True) -> pd.DataFrame:\n",
    "    \"\"\"Send query to narrative log or exposure log services.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    API_ENDPOINT : `str`\n",
    "        The URL to send the query to.\n",
    "        Usually like `https://usdf-rsp.slac.stanford.edu/narrativelog/messages`\n",
    "    params : `dict`\n",
    "        Dictionary of parameters for the REST API query.\n",
    "        See docs for each service for more details.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    messages : `pd.DataFrame`\n",
    "        The returned log messages (if any available), in a dataframe.\n",
    "    \"\"\"\n",
    "    # Very often, requests from the logging endpoints fail the first time.\n",
    "    response = requests.get(API_ENDPOINT, params)\n",
    "    # Try twice.\n",
    "    if response.status_code != 200:\n",
    "        response = requests.get(API_ENDPOINT, params)\n",
    "    if response.status_code != 200:\n",
    "        err_string = f\"{API_ENDPOINT} \"\n",
    "        err_string += \" unavailable.\"\n",
    "        print(err_string)\n",
    "        messages = []\n",
    "    else:\n",
    "        messages = response.json()\n",
    "    messages = pd.DataFrame(messages)\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c9f44b-5c40-49f0-834b-8b78ce785c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query any EFD topic for the timespan day_obs_min to day_obs_max, when you don't already know the fields\n",
    "# topic = lsst.sal.ScriptQueue.command_add\n",
    "# fields = await efd_client.get_fields(topic)\n",
    "# fields = [f for f in fields if 'private' not in f and f != 'name' and f!= \"duration\"]\n",
    "# dd = await efd_client.select_time_series(topic, fields, tstart, tend)\n",
    "# or top 5 .. \n",
    "# dd = await efd_client.select_top_n(topic, fields, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bc2c0c-0468-43ef-a276-2d1ac7763cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_script_stream(t_start: Time, t_end: Time, efd_client: EfdClient) -> pd.DataFrame:\n",
    "    \"\"\"Get script description and configuration from lsst.sal.Script.logevent_description\n",
    "    and lsst.sal.Script.command_configure topics.\n",
    "    \"\"\"\n",
    "    # Script will find information about how scripts are configured. \n",
    "    # The description topic gives a more succinct human name to the scripts\n",
    "    topic = 'lsst.sal.Script.logevent_description'\n",
    "    fields = ['classname', 'description', 'salIndex']\n",
    "    scriptdescription = await efd_client.select_time_series(topic, fields, t_start, t_end)\n",
    "    scriptdescription.rename({'salIndex': 'script_salIndex'}, axis=1, inplace=True)\n",
    "            \n",
    "    # This gets us more information about the script parameters, how they were configured\n",
    "    topic = 'lsst.sal.Script.command_configure'\n",
    "    fields = ['blockId', 'config',' executionId', 'salIndex']\n",
    "    fields = await efd_client.get_fields(topic)\n",
    "    fields = [f for f in fields if 'private' not in f]\n",
    "    # note blockId is only filled for JSON BLOCK activities\n",
    "    scriptconfig = await efd_client.select_time_series(topic, fields, t_start, t_end)\n",
    "    scriptconfig.rename({'salIndex': 'script_salIndex'}, axis=1, inplace=True)\n",
    "\n",
    "    # Merge these together on script_salIndex which is unique over tinterval\n",
    "    # Found that (command_configure - script description) index time is mostly << 1 second for each script and < 1 second over a night\n",
    "    script_stream = pd.merge(scriptdescription, scriptconfig, on='script_salIndex', suffixes=['_d', '_r'])\n",
    "    return script_stream\n",
    "\n",
    "\n",
    "async def get_script_state(t_start: Time, t_end: Time, queueIndex: int | None, efd_client: EfdClient) -> pd.DataFrame:\n",
    "    \"\"\"Get script status from lsst.sal.ScriptQueue.logevent_script topic.\"\"\"\n",
    "    # The status of each of these scripts is stored in scriptQueue.logevent_script\n",
    "    # so find the status of each of these scripts (this is status at individual stages).\n",
    "    topic = 'lsst.sal.ScriptQueue.logevent_script'\n",
    "    fields = await efd_client.get_fields(topic)\n",
    "    fields = ['blockId', 'path', 'processState', 'scriptState', 'salIndex', 'scriptSalIndex', \n",
    "             'timestampProcessStart', 'timestampConfigureStart', 'timestampConfigureEnd', 'timestampRunStart', 'timestampProcessEnd']\n",
    "    # Providing an integer salIndex will restrict this query to a single queue, but None will query all queues.\n",
    "    scripts = await efd_client.select_time_series(topic, fields, t_start, t_end, index=queueIndex)\n",
    "    scripts.rename({'scriptSalIndex': 'script_salIndex'}, axis=1, inplace=True)\n",
    "\n",
    "    # Group scripts on 'script_salIndex' to consolidate the information about its status stages\n",
    "    # Make a new column which we will fill with the max script state (== final state, given enum)\n",
    "    # (new column so we don't have to deal with multi-indexes from multiple aggregation methods)\n",
    "    scripts['finalScriptState'] = scripts['scriptState']\n",
    "    script_status = scripts.groupby('script_salIndex').agg({'path': 'first', \n",
    "                                                              'salIndex': 'max', \n",
    "                                                              'finalScriptState': 'max', \n",
    "                                                              'scriptState': 'unique', \n",
    "                                                              'processState': 'unique', \n",
    "                                                              'timestampProcessStart': 'min', \n",
    "                                                              'timestampConfigureStart': 'min', \n",
    "                                                              'timestampConfigureEnd': 'max', \n",
    "                                                              'timestampRunStart': 'max', \n",
    "                                                              'timestampProcessEnd': 'max'})\n",
    "    # Convert timestamp columns from unix_tai timestamps for readability.\n",
    "    # Yes, these timestamps really are unix_tai. \n",
    "    for col in [c for c in script_status.columns if c.startswith('timestamp')]:\n",
    "        script_status[col] = Time(script_status[col], format='unix_tai').utc.datetime\n",
    "    # Apply ScriptState enum for readability of final state\n",
    "    script_status['finalScriptState'] = script_status.apply(apply_enum, args=['finalScriptState', ScriptState], axis=1)\n",
    "    # Will apply 'best time' index after merge with script_stream\n",
    "    return script_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acdbdab-df7f-43fc-a678-e0039a33a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_script_status(t_start: Time, t_end: Time, efd_client: EfdClient) -> pd.DataFrame:\n",
    "    \"\"\"Given a start and end time, appropriately query each ScriptQueue to find \n",
    "    script descriptions, configurations and status.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t_start : `astropy.Time`\n",
    "        The time to start searching for script events.\n",
    "    t_end : `astropy.Time`\n",
    "        The time at which to end searching for script events.\n",
    "    efd_client : `EfdClient`\n",
    "        EfdClient to query the efd.\n",
    "    obsenv_client: `EfdClient`\n",
    "        EfdClient to query the obsenv (different database).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    script_status : `pd.DataFrame`\n",
    "        DataFrame containing script description, configuration, timing information and states.\n",
    "\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    The index of the returned dataframe is chosen from the timestamps recorded for the script. \n",
    "    In order to best place the script message inline with other events such as acquired images,\n",
    "    the time used is the `timestampRunStart` if available, `timestampConfigureEnd` next, and\n",
    "    then falls back to `timestampConfigureStart` or `timestampProcessStart` if those are also not\n",
    "    available.\n",
    "    \"\"\"\n",
    "\n",
    "    # The script_salIndex is ONLY unique during the time that a particular queue remains not OFFLINE\n",
    "    # However, each queue can go offline independently, so the time intervals that are required for each queue\n",
    "    # can be different, and requires inefficient querying of the lsst.sal.Script topics (which don't include \n",
    "    # the queue identification explicitly). Furthermore, the downtime is infrequent, so probably we'd\n",
    "    # most of the time prefer to do the efficient thing and query everything all at once. \n",
    "\n",
    "    # So first - see if that's possible.\n",
    "    topic = 'lsst.sal.ScriptQueue.logevent_summaryState'\n",
    "    fields = ['salIndex', 'summaryState']\n",
    "    # Were there breaks in this queue?\n",
    "    dd = await efd_client.select_time_series(topic, fields, t_start, t_end)\n",
    "    if len(dd) == 0:\n",
    "        offline_events = 0\n",
    "    else:\n",
    "        offline_state = CSCState.OFFLINE.value\n",
    "        offline_events = len(dd.query('summaryState == @offline_state'))\n",
    "    \n",
    "    if offline_events == 0:\n",
    "        print(f\"No OFFLINE events during time interval {t_start} to {t_end} for any queue.\")\n",
    "        # So then go ahead and just do a single big query.\n",
    "        script_stream = await get_script_stream(t_start, t_end, efd_client)\n",
    "        script_status = await get_script_state(t_start, t_end, None, efd_client)\n",
    "        script_status = pd.merge(script_stream, script_status, left_on='script_salIndex', right_index=True, suffixes=['', '_s'])\n",
    "    \n",
    "    else:\n",
    "        # The ScriptQueues can be started and stopped independently, so run needs to run per-scriptqueue, per-uptime\n",
    "        script_status = []\n",
    "        for queue in SalIndex:\n",
    "            topic = 'lsst.sal.ScriptQueue.logevent_summaryState'\n",
    "            fields = ['salIndex', 'summaryState']\n",
    "            # Were there breaks in this particular queue?\n",
    "            dd = await efd_client.select_time_series(topic, fields, t_start, t_end, index=queue)\n",
    "            if len(dd) == 0:\n",
    "                tstops = []\n",
    "                tintervals = [[t_start, t_end]]\n",
    "            else:\n",
    "                dd['state'] = dd.apply(apply_enum, args=['summaryState', CSCState], axis=1)\n",
    "                dd['state_time'] = Time(dd.index.values)\n",
    "            \n",
    "                tstops = dd.query('state == \"OFFLINE\"').state_time.values\n",
    "                if len(tstops) == 0:\n",
    "                    tintervals = [[t_start, t_end]]\n",
    "                if len(tstops) > 0:\n",
    "                    ts = tstops[0]\n",
    "                    ts_next = ts + TimeDelta(EPS_TIME)\n",
    "                    ts_next = Time(ts_next)\n",
    "                    tintervals = [[t_start, ts]]    \n",
    "                    for ts in tstops[1:]:\n",
    "                        tintervals.append([ts_next, ts])\n",
    "                        ts_next = ts + TimeDelta(EPS_TIME)\n",
    "                    tintervals.append([ts_next, t_end])\n",
    "            if len(tstops) == 0:\n",
    "                print(f\"For {queue.name}, found 0 ScriptQueue OFFLINE events in the time period  {t_start} to {t_end}.\")\n",
    "            else:\n",
    "                print(f\"For {queue.name}, found {len(tstops)} ScriptQueue restarts in the time period {t_start} to {t_end}, so will query in {len(tstops)+1} chunks\")\n",
    "            \n",
    "            # Do the script queue queries for each time interval in this queue\n",
    "            for tinterval in tintervals:\n",
    "                script_stream_t = await get_script_stream(tinterval[0], tinterval[1], efd_client)    \n",
    "                script_status_t = await get_script_state(tinterval[0], tinterval[1], queue, efd_client)\n",
    "                # Merge with script_stream so we get better descriptions and configuration information\n",
    "                dd = pd.merge(script_stream_t, script_status_t, left_on='script_salIndex', right_index=True, suffixes=['', '_s'])\n",
    "                script_status.append(dd)\n",
    "                print(f\"Found {len(dd)} script-status messages during {[e.iso for e in tinterval]} for {queue.name}\")\n",
    "        # Convert to a single dataframe\n",
    "        script_status = pd.concat(script_status)\n",
    "    \n",
    "    print(f\"Found {len(script_status)} script status messages\")\n",
    "    \n",
    "    # script_status columns: \n",
    "    # ['classname', 'description', 'script_salIndex', 'ScriptID', 'blockId',\n",
    "    # 'config', 'executionId', 'logLevel', 'pauseCheckpoint',\n",
    "    # 'stopCheckpoint', 'path', 'salIndex', 'finalScriptState', 'scriptState',\n",
    "    # 'processState', 'timestampProcessStart', 'timestampConfigureStart',\n",
    "    # 'timestampConfigureEnd', 'timestampRunStart', 'timestampProcessEnd'] \n",
    "    # columns used in final merged dataframe:\n",
    "    # ['time', 'name', 'description', 'config', 'script_salIndex', 'salIndex', \n",
    "    # 'finalStatus', 'timestampProcessStart', 'timestampConfigureEnd', 'timestampRunStart', 'timestampProcessEnd'] \n",
    "\n",
    "    def _find_best_script_time(x):\n",
    "        # Try run start first\n",
    "        best_time = x.timestampRunStart\n",
    "        if best_time == TIMESTAMP_ZERO:\n",
    "            best_time = x.timestampConfigureEnd\n",
    "        if best_time == TIMESTAMP_ZERO:\n",
    "            best_time = x.timestampConfigureStart\n",
    "        if best_time ==  TIMESTAMP_ZERO:\n",
    "            best_time = x.timestampProcessStart\n",
    "        return best_time    \n",
    "    # Create an index that will slot this into the proper place for runtime / image acquisition, etc\n",
    "    script_status.index = script_status.apply(_find_best_script_time, axis=1)\n",
    "    script_status.index = script_status.index.tz_localize(\"UTC\")\n",
    "    \n",
    "    script_status = pd.concat([script_status, dd])\n",
    "    script_status.sort_index(inplace=True)\n",
    "    return script_status\n",
    "\n",
    "\n",
    "async def get_scheduler_configs(t_start: Time, t_end: Time, efd_client: EfdClient, obsenv_client: EfdClient | None) -> pd.DataFrame:\n",
    "    # Scheduler dependency information\n",
    "    t_start_local = t_start\n",
    "    topic = 'lsst.sal.Scheduler.logevent_dependenciesVersions'\n",
    "    fields = await efd_client.get_fields(topic)\n",
    "    fields = [f for f in fields if \"private\" not in f]\n",
    "    deps = await efd_client.select_time_series(topic, fields, t_start_local, t_end)\n",
    "    # Sometimes the scheduler hasn't been set up, if it's a limited timespan.\n",
    "    if len(deps) == 0:\n",
    "        t_start_local = t_start - TimeDelta(1, format='jd')\n",
    "        deps = await efd_client.select_time_series(topic, fields, t_start_local, t_end)\n",
    "        deps = deps.iloc[:1]\n",
    "    # Reconfigure output to fit into script_status fields \n",
    "    deps['classname'] = \"Scheduler dependencies\"\n",
    "    deps['description'] = deps['scheduler'] + ' ' + deps['seeingModel']\n",
    "    models = [c for c in deps.columns if 'observatory' in c or 'Model' in c]\n",
    "    def build_dep_string(x, models): \n",
    "        dep_string = ''\n",
    "        for m in models:\n",
    "            dep_string += f\"{m}: {x[m]}, \"\n",
    "        dep_string = dep_string[:-2]\n",
    "        return dep_string\n",
    "    deps['config'] = deps.apply(build_dep_string, args=[models], axis=1)\n",
    "    deps['script_salIndex'] = -1\n",
    "    \n",
    "    # And within Scheduler, what is ts_config_ocs and scripts versions\n",
    "    # Need find the previous version of tc_config_ocs \n",
    "    topic = 'lsst.obsenv.summary'\n",
    "    fields = ['summit_extras', 'ts_standardscripts', 'ts_externalscripts', 'ts_config_ocs']\n",
    "    # Query longer time period for obsenv, so we can be sure to know how scheduler enables\n",
    "    obsenv = await obsenv_client.select_time_series(topic, fields, t_start_local - TimeDelta(1, format='jd'), t_end)\n",
    "    fields = ['summit_extras', 'ts_standardscripts', 'ts_externalscripts', 'ts_config_ocs']\n",
    "    check = np.all((obsenv[fields][1:].values == obsenv[fields][:-1].values), axis=1)\n",
    "    classname = np.where(check, \"Obsenv Check\", \"Obsenv Update\")\n",
    "    obsenv['classname'] = np.concatenate([np.array(['Obsenv']), classname])\n",
    "    obsenv['description'] = (\"ts_config_ocs: \" + obsenv['ts_config_ocs'] + \n",
    "                            \" summit_extras: \" + obsenv['summit_extras'])\n",
    "    obsenv['config'] = (\"ts_standardscripts: \" + obsenv['ts_standardscripts'] + \n",
    "                        \" ts_externalscripts: \" + obsenv['ts_externalscripts'])\n",
    "    obsenv['salIndex'] = 1\n",
    "    obsenv['script_salIndex'] = -1\n",
    "    \n",
    "    # I think these should be every time scheduler is \"ENABLED\"\n",
    "    topic = 'lsst.sal.Scheduler.logevent_configurationApplied'\n",
    "    fields = await efd_client.get_fields(topic)\n",
    "    fields = [f for f in fields if \"private\" not in f]\n",
    "    con = await efd_client.select_time_series(topic, fields, t_start_local, t_end)\n",
    "    con['classname'] = \"Scheduler configuration\"\n",
    "    # Build description from schemaVersion (just in case) and ts_config_ocs \n",
    "    ts_config_ocs_in_place = []\n",
    "    for time in con.index:\n",
    "        prev_obsenv = obsenv.query('index < @time')\n",
    "        if len(prev_obsenv) == 0:\n",
    "            ts_config_ocs_in_place.append('Unknown')\n",
    "        else:\n",
    "            ts_config_ocs_in_place.append(prev_obsenv.iloc[-1]['ts_config_ocs'])\n",
    "    con['ts_config_ocs'] = ts_config_ocs_in_place\n",
    "    con['description'] = 'ts_config_ocs ' + con['ts_config_ocs'] + ' ' + con['schemaVersion']\n",
    "    con.rename({'configurations': 'config'}, axis=1, inplace=True)\n",
    "    con['script_salIndex'] = -1\n",
    "\n",
    "    # Combine results\n",
    "    dd =  pd.concat([deps, con, obsenv])\n",
    "    # Trim back results to t_start, keeping last previous update information\n",
    "    # Trim obsenv back to range for other values\n",
    "    # But keep last entry so we have easy record \n",
    "    tt = pd.to_datetime(t_start.utc.datetime).tz_localize(\"UTC\")\n",
    "    # Keep last scheduler configuration update\n",
    "    old_dd_sched = dd.query('index < @tt and classname == \"Scheduler configuration\"')[-1:]\n",
    "    old_dd_deps = dd.query('index < @tt and classname == \"Scheduler dependencies\"')[-1:]\n",
    "    old_dd_obsenv = dd.query('index < @tt and classname.str.contains(\"Obsenv\")')[-1:]\n",
    "    dd = dd.query('index >= @tt')\n",
    "    sched_config = pd.concat([old_dd_sched, old_dd_obsenv, old_dd_deps, dd])\n",
    "\n",
    "    # Reformat\n",
    "    cols = ['classname', 'description', 'config', 'salIndex', 'script_salIndex']\n",
    "    drop_cols = [c for c in sched_config.columns if c not in cols]\n",
    "    sched_config.drop(drop_cols, axis=1, inplace=True)\n",
    "    sched_config.sort_index(inplace=True)\n",
    "    sched_config['timestampProcessStart'] = sched_config.index.copy().tz_localize(None).astype('datetime64[ns]')\n",
    "    sched_config['finalScriptState'] = \"Configuration\"\n",
    "    print(f\"Found {len(sched_config)} scheduler configuration records\")\n",
    "    return sched_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c215f3-9024-4339-8fde-d970b2764757",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_error_codes(t_start: Time, t_end: Time, efd_client: EfdClient) -> pd.DataFrame:\n",
    "    \"\"\"Get all messages from logevent_errorCode topics.\"\"\"\n",
    "    # Get error codes\n",
    "    topics = await efd_client.get_topics()\n",
    "    err_codes = [t for t in topics if 'errorCode' in t]\n",
    "    \n",
    "    errs = []\n",
    "    for topic in err_codes:\n",
    "        df = await efd_client.select_time_series(topic, ['errorCode', 'errorReport'], t_start, t_end)\n",
    "        if len(df) > 0:\n",
    "            df['topic'] = topic\n",
    "            errs += [df]\n",
    "    if len(errs) > 0:\n",
    "        errs = pd.concat(errs).sort_index()\n",
    "        def strip_csc(x):\n",
    "            return x.topic.replace(\"lsst.sal\", \"\").replace(\"logevent_errorCode\", \"\").replace(\".\", \"\") + \"CSC error\"\n",
    "        errs['component'] = errs.apply(strip_csc, axis=1)\n",
    "        # Rename some columns to match narrative log columns\n",
    "        errs.rename({'errorCode': 'error_code', 'errorReport': 'message_text', 'topic': 'origin'}, axis=1, inplace=True)\n",
    "        # Add a salindex so we can color-code based on this as a \"source\"\n",
    "        errs['salIndex'] = 4\n",
    "        errs['finalStatus'] = \"ERR\"\n",
    "        errs['timestampProcessStart'] = errs.index.values.copy()\n",
    "    \n",
    "    print(f\"Found {len(errs)} error messages\")\n",
    "    return errs\n",
    "\n",
    "async def get_watcher_alarms(t_start: Time, t_end: Time, efd_client: EfdClient) -> pd.DataFrame:\n",
    "    \"\"\"Get and consolidate watcher alarms from lsst.sal.Watcher.logevent_alarm topic.\"\"\"\n",
    "    topic = 'lsst.sal.Watcher.logevent_alarm'\n",
    "    fields = await efd_client.get_fields(topic)\n",
    "    fields = [f for f in fields if ('private' not in f) and (f != 'name') and (f != 'duration')]\n",
    "    watcher_messages = await efd_client.select_time_series(topic, fields, t_start, t_end)\n",
    "    # Convert severity to readable string.\n",
    "    watcher_messages['severity'] = watcher_messages.apply(apply_enum, args=('severity', AlarmSeverity), axis=1)\n",
    "    # Convert times for readability.\n",
    "    for col in [c for c in watcher_messages.columns if 'timestamp' in c]:\n",
    "        watcher_messages[col] = Time(watcher_messages[col], format='unix_tai').utc.datetime\n",
    "    # Join on reason to consolidate messages, then join on timestampAcknowledged and timestampSeverityOldest\n",
    "    watcher_messages = watcher_messages.groupby(['reason', 'timestampAcknowledged']).first() \n",
    "    watcher_messages.reset_index(drop=False, inplace=True)\n",
    "    # Join watcher messages based on timestampSeverityOldest too, maybe\n",
    "    watcher_messages = watcher_messages.groupby('timestampSeverityOldest').first()\n",
    "    watcher_messages.reset_index(drop=False, inplace=True)\n",
    "    watcher_messages.index = watcher_messages['timestampSeverityOldest'].copy()\n",
    "    watcher_messages.index.names = [None]\n",
    "    watcher_messages.index = watcher_messages.index.tz_localize(\"UTC\")\n",
    "    # And since the timestampSeverityOldest can be different while the future \n",
    "    # Rename some columns for merge with errors \n",
    "    watcher_messages.rename({'reason': 'message_text', 'escalateTo': 'component',  'acknowledgedBy': 'origin', 'severity': 'error_code'}, axis=1, inplace=True)\n",
    "    watcher_messages['salIndex'] = 4\n",
    "    watcher_messages['error_code'] = 0\n",
    "    watcher_messages['finalStatus'] = \"ALARM\"\n",
    "    print(f\"Found {len(watcher_messages)} watcher messages\")\n",
    "    return watcher_messages.sort_index()\n",
    "\n",
    "\n",
    "def get_narrative_log(t_start: Time, t_end: Time, narrative_log_endpoint: str) -> pd.DataFrame:\n",
    "    \"\"\"Get the narrative log entries.\"\"\"    \n",
    "    log_limit = 50000\n",
    "    params = {\"is_human\" : \"either\",\n",
    "              \"is_valid\" : \"true\",\n",
    "              \"has_date_begin\" : True,\n",
    "              \"min_date_begin\" : t_start.to_datetime(),\n",
    "              \"max_date_begin\" : t_end.to_datetime(),\n",
    "              \"order_by\" : \"date_begin\",\n",
    "              \"limit\": log_limit, \n",
    "             }\n",
    "    messages = query_logging_services(narrative_log_endpoint, params)\n",
    "    # Modify narrative log content to match dataframes from errors and watcher topics better.\n",
    "    # Strip out repeated \\n\\n and \\r\\n characters for nicer printing in dataframe.\n",
    "    if len(messages) > 0:\n",
    "        def strip_rns(x):\n",
    "            return x.message_text.replace(\"\\r\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\").rstrip(\"\\n\")\n",
    "        def make_time(x, column):\n",
    "            return Time(x[column], format='isot', scale='tai').utc.datetime\n",
    "        def clarify_log(x):\n",
    "            if x.components is None:\n",
    "                component = \"Log\"\n",
    "            else:\n",
    "                component = \"Log \" + \" \".join(x.components)\n",
    "            return component\n",
    "        # Strip excessive \\r\\n and \\n\\n from messages\n",
    "        messages['message_text'] = messages.apply(strip_rns, axis=1)\n",
    "        # Add a time index\n",
    "        messages['time'] = messages.apply(make_time, args=[\"date_begin\"], axis=1)\n",
    "        messages.set_index('time', inplace=True)\n",
    "        messages.index = messages.index.tz_localize(\"UTC\")\n",
    "        # Join the components and add \"Log\" explicitly\n",
    "        messages['component'] = messages.apply(clarify_log, axis=1)\n",
    "        # rename some columns to match error data\n",
    "        messages.rename({'time_lost_type': 'error_code', 'user_id': 'origin'}, axis=1, inplace=True)\n",
    "        # Add a salindex so we can color-code based on this as a \"source\"\n",
    "        messages['salIndex'] = 0\n",
    "        messages['error_code'] = 0\n",
    "        messages['finalStatus'] = \"Log\"\n",
    "        messages['timestampProcessStart'] = messages.apply(make_time, args=[\"date_begin\"], axis=1)\n",
    "        messages['timestampRunStart'] = messages.apply(make_time, args=[\"date_added\"], axis=1)\n",
    "        messages['timestampProcessEnd'] = messages.apply(make_time, args=[\"date_end\"], axis=1)\n",
    "    print(f\"Found {len(messages)} messages in the narrative log\")\n",
    "    if len(messages) == log_limit:\n",
    "        print(f\"Whoops, likely lost some log messages due to limit of {log_limit}.\")\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8458d9d-b7e8-44a3-a771-bd7aff30cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_narrative_and_errs(t_start: Time, t_end: Time, efd_client: EfdClient, narrative_log_endpoint: str | None, include_watcher : bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Get narrative, errorCode and (possibly) watcher alarms.\"\"\"\n",
    "\n",
    "    if narrative_log_endpoint is not None:\n",
    "        messages = get_narrative_log(t_start, t_end, narrative_log_endpoint)\n",
    "    else:\n",
    "        messages = pd.DataFrame([])\n",
    "        \n",
    "    errs = await get_error_codes(t_start, t_end, efd_client)\n",
    "    if include_watcher:\n",
    "        watcher = await get_watcher_alarms(t_start,  t_end, efd_client)\n",
    "    else:\n",
    "        # Maybe we'll get some of the messages, for start of the night state, for now\n",
    "        # watcher = await get_watcher_alarms(t_start,  t_end, efd_client)\n",
    "        # watcher = watcher.query('message_text.str.len() > 100')\n",
    "        watcher = pd.DataFrame([])\n",
    "        print(f\"Kept {len(watcher)} watcher messages\")\n",
    "        \n",
    "    # Merge narrative log messages and error messages    \n",
    "    narrative_and_errs = pd.concat([errs, watcher, messages]).sort_index()\n",
    "    ncols = ['component', 'origin', 'message_text', 'error_code', 'salIndex', 'timestampSeverityOldest', 'timestampAcknowledged', 'timestampMaxSeverity']\n",
    "    return narrative_and_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4c636-e972-411b-bf49-363e8e86e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_exposure_info(t_start: Time, t_end: Time, efd_client: EfdClient, exposure_log_endpoint: str | None) -> pd.DataFrame:\n",
    "    \"\"\"Get exposure information from lsst.sal.CCCamera.logevent_endOfImageTelemetry\n",
    "    and join it with exposure log information. \n",
    "    \"\"\"\n",
    "    # Find exposure information\n",
    "    topic = 'lsst.sal.CCCamera.logevent_endOfImageTelemetry' \n",
    "    fields = ['imageName', 'imageIndex', 'exposureTime', 'darkTime', 'measuredShutterOpenTime', 'additionalValues', 'timestampAcquisitionStart', 'timestampDateEnd', 'timestampDateObs']\n",
    "    image_acquisition = await efd_client.select_time_series(topic, fields, t_start, t_end)\n",
    "    for col in [c for c in image_acquisition.columns if c.startswith(\"timestamp\")]:\n",
    "        image_acquisition[col] = Time(image_acquisition[col], format='unix_tai').utc.datetime\n",
    "    image_acquisition['salIndex'] = -1\n",
    "    image_acquisition['script_salIndex'] = 0\n",
    "    image_acquisition['finalStatus'] = \"Image Acquired\"\n",
    "    def make_config_col_for_image(x):\n",
    "        return f\"exp {x.exposureTime} // dark {x.darkTime} // open {x.measuredShutterOpenTime} \"\n",
    "    image_acquisition['config'] = image_acquisition.apply(make_config_col_for_image, axis=1)\n",
    "    image_acquisition.index = image_acquisition['timestampAcquisitionStart'].copy()\n",
    "    image_acquisition.index = image_acquisition.index.tz_localize(\"UTC\")\n",
    "    print(f\"Found {len(image_acquisition)} image times\")\n",
    "\n",
    "    # Now get exposure log information if exposure_log_endpoint defined.\n",
    "    if exposure_log_endpoint is not None:\n",
    "        log_limit = 50000\n",
    "        # A cheap conversion to dayobs int\n",
    "        min_dayobs_int = int(t_start.iso[0:10].replace('-', ''))\n",
    "        max_dayobs_int = int(t_end.iso[0:10].replace('-', ''))\n",
    "        params = {\"is_human\" : \"either\",\n",
    "                  \"is_valid\" : \"true\",\n",
    "                  \"min_day_obs\" : min_dayobs_int,\n",
    "                  \"max_day_obs\" : max_dayobs_int,\n",
    "                  \"limit\": log_limit, \n",
    "                 }\n",
    "        \n",
    "        exp_logs = query_logging_services(exposure_log_endpoint, params)\n",
    "        print(f\"Found {len(exp_logs)} messages in the exposure log\")\n",
    "        \n",
    "        # Modify exposure log and match with exposures to add time tag.\n",
    "        if len(exp_logs) > 0:\n",
    "            # Find a time to add the exposure logs into the records (next to the image).\n",
    "            exp = pd.merge(image_acquisition, exp_logs, how='right', left_on='imageName', right_on='obs_id')\n",
    "            # Set the time for the exposure log just slightly after the image start time\n",
    "            exp_log_image_time = exp['timestampAcquisitionStart'] + EPS_TIME\n",
    "            exp_logs['img_time'] = exp_log_image_time\n",
    "            exp_logs.set_index('img_time', inplace=True)\n",
    "            exp_logs.index = exp_logs.index.tz_localize(\"UTC\")\n",
    "            exp_logs['salIndex'] = 0\n",
    "            exp_logs['script_salIndex'] = 0\n",
    "            # Rename some columns in the exposure log so that we can consolidate them here\n",
    "            exp_logs.rename({'obs_id': 'imageName', 'user_id': 'config', 'message_text': 'additionalValues', 'exposure_flag': 'finalStatus'}, axis=1, inplace=True)\n",
    "            image_acquisition = pd.concat([image_acquisition, exp_logs]).sort_index()\n",
    "            print(\"Joined exposure and exposure log\")\n",
    "    return image_acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d951f2f-db3f-42a3-9853-0e1a5a11716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lsst.summit.utils import ConsDbClient\n",
    "# # Not sure of summit consdb access, just use USDF for now\n",
    "# os.environ[\"LSST_CONSDB_PQ_URL\"] = \"http://consdb-pq.consdb:8080/consdb\"\n",
    "# os.environ[\"no_proxy\"] += \",.consdb\"\n",
    "\n",
    "# day_obs_int_min = int(day_obs_min.replace('-', ''))\n",
    "# day_obs_int_max = int(day_obs_max.replace('-', ''))\n",
    "\n",
    "# # Use the ConsDB Client, and add a couple of tries \n",
    "# consdb = ConsDbClient()\n",
    "\n",
    "# instrument = 'lsstcomcam'\n",
    "# visit_query = f'''\n",
    "#     SELECT * \n",
    "#     FROM cdb_{instrument}.visit1\n",
    "#      WHERE day_obs >= {day_obs_int_min}\n",
    "#      and day_obs  <= {day_obs_int_max}\n",
    "# '''\n",
    "# try:\n",
    "#     visits = consdb.query(visit_query).to_pandas()\n",
    "# except requests.HTTPError or requests.JSONDecodeError:\n",
    "#     # Try twice\n",
    "#     visits = consdb.query(visit_query).to_pandas()\n",
    "    \n",
    "# visits.set_index('visit_id', inplace=True)\n",
    "# (Time(visits.query('exposure_name == \"CC_O_20241115_000347\"')['exp_midpt'].values[0], format='isot', scale='tai') - TimeDelta(15 * u.second)).utc.iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dcdf84-7983-4b00-bedc-b2ad15e1869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_consolidated_messages(t_start: Time, t_end: Time, include_watcher: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Get consolidated messages from EFD ScriptQueue, errorCodes, CCCamera, exposure and narrative logs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t_start : `astropy.Time`\n",
    "        Time of the start of the messages.\n",
    "    t_end : `astropy.Time`\n",
    "        Time of the end of the messages.\n",
    "    include_watcher : `bool`\n",
    "        Include messages from Watcher.logevent_alarms?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    efd_and_messages : `pd.DataFrame`\n",
    "    \"\"\"\n",
    "        \n",
    "    endpoints = get_clients()\n",
    "    print(endpoints)\n",
    "\n",
    "    # Now rename columns so we can put these all into the same dataframe\n",
    "    # goal columns : \n",
    "    cols = ['time', 'name', 'description', 'config', 'script_salIndex', 'salIndex', 'finalStatus', 'timestampProcessStart', 'timestampConfigureEnd', 'timestampRunStart', 'timestampProcessEnd'] \n",
    "    \n",
    "    # columns from scripts\n",
    "    script_status = await get_script_status(t_start, t_end, endpoints['efd'])\n",
    "    # script_cols = ['classname', 'description', 'config', 'script_salIndex', 'salIndex', 'blockId', 'finalScriptState', 'scriptState', 'timestampProcessStart', 'timestampConfigureEnd', 'timestampRunStart', 'timestampProcessEnd']\n",
    "    scheduler_configs = await get_scheduler_configs(t_start, t_end, endpoints['efd'], endpoints['obsenv'])\n",
    "    script_status = pd.concat([scheduler_configs, script_status])\n",
    "    script_status.rename({'classname': 'name', 'finalScriptState': 'finalStatus'}, axis=1, inplace=True)\n",
    "    \n",
    "    # columns from narrative and errors\n",
    "    narrative_and_errs = await get_narrative_and_errs(t_start, t_end, endpoints['efd'], endpoints['narrative_log'], include_watcher=include_watcher)\n",
    "    # narrative_cols = ['component', 'origin', 'message_text', 'error_code', 'salIndex']\n",
    "    # if include_watcher:\n",
    "    #     narrative_cols = narrative_cols + ['timestampSeverityOldest', 'timestampAcknowledged', 'timestampMaxSeverity']\n",
    "    narrative_and_errs.rename({'component': 'name', 'origin': 'config', 'message_text': 'description', 'error_code': 'script_salIndex'}, axis=1, inplace=True)\n",
    "    if 'timestampSeverityOldest' in narrative_and_errs.columns:\n",
    "        narrative_and_errs.rename({'timestampSeverityOldest': 'timestampProcessStart', 'timestampAcknowledged': 'timestampConfigureEnd', 'timestampMaxSeverity': 'timestampRunStart'}, axis=1, inplace=True)\n",
    "    \n",
    "    # columns from images_and_logs\n",
    "    image_and_logs = await get_exposure_info(t_start, t_end, endpoints['efd'], endpoints['exposure_log'])\n",
    "    # image_cols = ['imageName', 'additionalValues', 'config', 'finalStatus', 'script_salIndex', 'salIndex', 'timestampAcquisitionStart', 'timestampDateObs', 'timestampDateEnd']\n",
    "    image_and_logs.rename({'imageName': 'name', 'additionalValues' : 'description', \n",
    "                           'timestampAcquisitionStart': 'timestampProcessStart', 'timestampDateObs': 'timestampRunStart', 'timestampDateEnd': 'timestampProcessEnd'}, axis=1, inplace=True) \n",
    "    \n",
    "    efd_and_messages = pd.concat([script_status, narrative_and_errs, image_and_logs]).sort_index()\n",
    "    # Wrap description, which can may have long zero-space messages in the errors\n",
    "    efd_and_messages['description'] = efd_and_messages['description'].str.wrap(100)\n",
    "    # use an integer index, which makes it easier to pull up values plus avoids occasional failures of time uniqueness\n",
    "    efd_and_messages.reset_index(drop=False, inplace=True)\n",
    "    efd_and_messages.rename({'index': 'time'}, axis=1, inplace=True)\n",
    "\n",
    "    print(f\"Total combined messages {len(efd_and_messages)}\")\n",
    "    return efd_and_messages, cols\n",
    "\n",
    "# Add a custom formatter to handle YAML-like strings with dynamic background colors\n",
    "def format_config_as_yaml_with_colors(row):\n",
    "    config_value = row['config']\n",
    "    sal_index = row['salIndex']\n",
    "    script_salindex = row['script_salIndex']\n",
    "    \n",
    "    # Define background colors based on salIndex\n",
    "    background_colors = {\n",
    "        1: '#b4c546',  # Simonyi queue\n",
    "        2: '#bab980',  # Aux Tel queue\n",
    "        3: '#b2baad',  # OCS queue\n",
    "    }\n",
    "    # Default color if salIndex doesn't match any condition\n",
    "    background_color = background_colors.get(sal_index, '#f9f9f9')\n",
    "    \n",
    "    if script_salindex > 0 and sal_index in [1,2,3] and isinstance(config_value, str):\n",
    "        try:\n",
    "            # Parse the YAML-like string\n",
    "            parsed_yaml = yaml.safe_load(config_value)\n",
    "            # Format back to YAML with proper indentation\n",
    "            formatted_yaml = yaml.dump(parsed_yaml, default_flow_style=False)\n",
    "            return (\n",
    "                f\"<pre style='background: {background_color}; padding: 10px; border: 1px solid #ddd; margin: 0;'>\"\n",
    "                f\"{formatted_yaml}</pre>\"\n",
    "            )\n",
    "        except yaml.YAMLError:\n",
    "            # If parsing fails, return as plain text in a styled <pre> block\n",
    "            return (\n",
    "                f\"<pre style='background: {background_color}; padding: 10px; border: 1px solid #ddd; margin: 0;'>\"\n",
    "                f\"{config_value}</pre>\"\n",
    "            )\n",
    "    else:\n",
    "        return config_value  # Return as-is if salIndex is 0 or invalid type\n",
    "    \n",
    "\n",
    "def pretty_print_messages(efd_and_messages: pd.DataFrame, cols: list, time_order: str) -> None:\n",
    "    \n",
    "    def highlight_salindex(s):\n",
    "        # Colors from https://medialab.github.io/iwanthue/\n",
    "        if s.salIndex == 0:     # narrative log\n",
    "            return ['background-color: #cf7ddc'] * len(s)\n",
    "        elif s.salIndex ==  4:  # error messages\n",
    "            return ['background-color: #9cb5d5'] * len(s)\n",
    "        elif s.salIndex == 1:   # simonyi queue\n",
    "            return ['background-color: #b4c546'] * len(s)\n",
    "        elif s.salIndex == 2:   # aux tel queue\n",
    "            return ['background-color: #bab980'] * len(s)\n",
    "        elif s.salIndex == 3:   # ocs queue\n",
    "            return ['background-color: #b2baad'] * len(s)\n",
    "        #elif s.salIndex == -1:  # image\n",
    "        #    return ['background-color: #b6ecf5'] * len(s)\n",
    "        else:\n",
    "            return [''] * len(s)\n",
    "    \n",
    "    print(\"Color coding by salIndex (1/2/3 scriptqueue index) + data source (narrative or exposure log, or EFD logevent_errorCode messages)\")\n",
    "    print('')\n",
    "    \n",
    "    if time_order == \"newest first\": \n",
    "        efd_and_messages = efd_and_messages[::-1]\n",
    "\n",
    "    # Apply yaml-like formatting conditionally\n",
    "    efd_and_messages['config'] = efd_and_messages.apply(format_config_as_yaml_with_colors, axis=1)\n",
    "    \n",
    "    # Adjust the display call to include the formatted column\n",
    "    styled_table = (\n",
    "        efd_and_messages[cols]\n",
    "        .style.apply(highlight_salindex, axis=1)  # Preserve color formatting for other columns\n",
    "        .set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "        .set_properties(**{'text-align': 'left'})\n",
    "    )\n",
    "    \n",
    "    # Render with HTML\n",
    "    display(HTML(styled_table.format().to_html()))\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e1344-c54e-448b-abbe-4d2dda3e9011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a range of times to search, based on dayobs\n",
    "if day_obs_min == \"Today\":\n",
    "    # Shift the 12hour offset following the definition of day_obs in https://sitcomtn-032.lsst.io/    \n",
    "    # Drop the hours, minutes, seconds to get the ISO formatted day_obs\n",
    "    day_obs_min = Time(np.floor(Time.now().mjd - 0.5), format='mjd', scale='utc').iso[0:10]\n",
    "\n",
    "if day_obs_min == \"Yesterday\":\n",
    "    # Shift the 12hour offset following the definition of day_obs in https://sitcomtn-032.lsst.io/\n",
    "    # Drop the hours, minutes, seconds to get the ISO fromatted day_obs\n",
    "    day_obs_min = (Time(np.floor(Time.now().mjd - 0.5), format='mjd', scale='utc') - TimeDelta(1, format='jd')).iso[0:10]\n",
    "\n",
    "\n",
    "# Set a range of times to search, based on dayobs\n",
    "if day_obs_max == \"Today\":\n",
    "    # Shift the 12hour offset following the definition of day_obs in https://sitcomtn-032.lsst.io/    \n",
    "    # Drop the hours, minutes, seconds to get the ISO formatted day_obs\n",
    "    day_obs_max = Time(np.floor(Time.now().mjd - 0.5), format='mjd', scale='utc').iso[0:10]\n",
    "\n",
    "if day_obs_max == \"Yesterday\":\n",
    "    # Shift the 12hour offset following the definition of day_obs in https://sitcomtn-032.lsst.io/\n",
    "    # Drop the hours, minutes, seconds to get the ISO fromatted day_obs\n",
    "    day_obs_max = (Time(np.floor(Time.now().mjd - 0.5), format='mjd', scale='utc') - TimeDelta(1, format='jd')).iso[0:10]\n",
    "\n",
    "try:\n",
    "    t_start = Time(f\"{day_obs_min}T12:00:00\", format='isot', scale='utc')\n",
    "except ValueError:\n",
    "    print(f\"Is day_obs_min the right format? {day_obs_min} should be YYYY-MM-DD\")\n",
    "    t_start = None\n",
    "try:\n",
    "    t_end = Time(f\"{day_obs_max}T12:00:00\", format='isot', scale='utc') + TimeDelta(1, format='jd')\n",
    "except ValueError:\n",
    "    print(f\"Is day_obs_max the right format? {day_obs_max} should be YYYY-MM-DD\")\n",
    "    t_start = None\n",
    "\n",
    "if t_start is None or t_end is None:\n",
    "    print(\"Did not get valid inputs for time period.\")\n",
    "\n",
    "\n",
    "print(f\"Querying for messages from {t_start.iso} to {t_end.iso}\")\n",
    "print(f\"Notebook executed at {Time.now().utc.iso}\")\n",
    "efd_and_messages, cols = await get_consolidated_messages(t_start, t_end)\n",
    "\n",
    "# Could add these to parameters\n",
    "save_log = False\n",
    "make_link = False\n",
    "\n",
    "if save_log:\n",
    "    log_filename = f\"log_{day_obs_min}_{day_obs_max}.h5\"\n",
    "    # We will always get a performance warning here, because the dataframe includes string objects\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        efd_and_messages[cols].to_hdf(log_filename, key='messages')\n",
    "        print(f\"Wrote to {log_filename}\")\n",
    "if make_link:\n",
    "    import base64\n",
    "    html_table = efd_and_messages[cols].to_xml(index=False)\n",
    "    b64 = base64.b64encode(html_table.encode())\n",
    "    payload = b64.decode()\n",
    "    log_xml =  f\"log_{day_obs_min}_{day_obs_max}.xml\"\n",
    "    html_link = f'<a download=\"{log_xml}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">Download XML table of log messages</a>'\n",
    "    display(HTML(html_link))\n",
    "    print(\" read download with pandas.read_xml, convert times using .astype('datetime64[ns]')\")\n",
    "        \n",
    "pretty_print_messages(efd_and_messages, cols, time_order)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

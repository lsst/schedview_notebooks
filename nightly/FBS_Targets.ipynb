{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1433594-d951-416a-bf43-3068e46f26cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is only for setting example parameter defaults - gets replaced by sidecar.\n",
    "day_obs = \"2024-11-08\"\n",
    "#day_obs = \"2024-06-27\"\n",
    "#day_obs = \"Today\"\n",
    "#telescope = \"AuxTel\"  \n",
    "telescope = \"SimonyiTel\"\n",
    "#instrument = \"latiss\"  \n",
    "instrument = \"lsstcomcam\"\n",
    "#salindex = 2\n",
    "salindex = 1\n",
    "timezone = \"Chile/Continental\"\n",
    "#timezone = \"UTC\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac5b1f-b036-4213-ae6d-700dcad8a7a6",
   "metadata": {},
   "source": [
    "# Scheduler: Targets, Observations and ConsDB Visits for {{ params.day_obs }} {{ params.telescope }}\n",
    "\n",
    "The Feature Based Scheduler requests `Targets` and completed observation scripts result in `Observations`; these are both are tracked in the EFD.\n",
    "The Consdb reports acquired `Visits`.  \n",
    "On-sky exposures can also be acquired directly through execution of scripts or JSON BLOCKs; these don't result in `Targets` but do produce `Visits`.\n",
    "\n",
    "\n",
    "* [Almanac](#Almanac)\n",
    "* [EFD Configuration](#EFD_configuration)\n",
    "* [EFD Targets and Observations](#EFD_targets)\n",
    "* [ConsDB Visits](#Consdb_visits)\n",
    "* [Overheads and Issues](#Overheads)\n",
    "* [Summary Plot](#Summary_plot)\n",
    "* [Night Log Report](#Night_report)\n",
    "* [Narrative and Exposure Log](#Narrative_exposure_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17629b1c-626e-417f-8bdc-20332107534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import copy\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import matplotlib.pylab as plt\n",
    "from cycler import cycler\n",
    "import colorcet as cc\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import option_context\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "import datetime\n",
    "from astropy.time import Time, TimeDelta\n",
    "import astropy.units as u\n",
    "import astropy\n",
    "astropy.utils.iers.conf.iers_degraded_accuracy = 'ignore'\n",
    "\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "try:\n",
    "    tz = ZoneInfo(timezone)\n",
    "    tz_utc = ZoneInfo(\"UTC\")\n",
    "except ZoneInfoNotFoundError:\n",
    "    print(\"Timezone should be a string recognizable to `ZoneInfo`.\")\n",
    "    print(\"Using Chile/Continental (+UTC) backup.\")\n",
    "    tz = ZoneInfo(\"Chile/Continental\")\n",
    "    tz_utc = ZoneInfo(\"UTC\")\n",
    "\n",
    "from rubin_scheduler.site_models import Almanac\n",
    "from rubin_scheduler.utils import Site\n",
    "\n",
    "import requests\n",
    "import urllib\n",
    "import sqlalchemy\n",
    "from lsst_efd_client import EfdClient\n",
    "try:\n",
    "    from lsst.summit.utils import ConsDbClient\n",
    "    have_consdb = True\n",
    "except ImportError:\n",
    "    have_consdb = False\n",
    "\n",
    "\n",
    "\n",
    "# for scheduler snapshots\n",
    "from urllib.parse import urlparse\n",
    "from lsst.resources import ResourcePath\n",
    "\n",
    "\n",
    "# at USDF or at summit?\n",
    "if os.getenv(\"EXTERNAL_INSTANCE_URL\", \"\") == \"https://summit-lsp.lsst.codes\":\n",
    "    efd = 'summit_efd'\n",
    "else:\n",
    "    efd = 'usdf_efd'  \n",
    "    os.environ[\"RUBIN_SIM_DATA_DIR\"] = \"/sdf/data/rubin/shared/rubin_sim_data\"\n",
    "# Not sure of summit consdb access, just use USDF for now\n",
    "os.environ[\"LSST_CONSDB_PQ_URL\"] = \"http://consdb-pq.consdb:8080/consdb\"\n",
    "os.environ[\"no_proxy\"] += \",.consdb\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eda54a-f5fa-4f1a-9068-af721e185471",
   "metadata": {},
   "outputs": [],
   "source": [
    "if day_obs == \"Today\":\n",
    "    # Shift the 12hour offset following the definition of day_obs in https://sitcomtn-032.lsst.io/    \n",
    "    # Drop the hours, minutes, seconds to get the ISO formatted day_obs\n",
    "    day_obs = (Time.now() - TimeDelta(0.5, format='jd')).iso[:10]\n",
    "\n",
    "elif day_obs == \"Yesterday\":\n",
    "    # Shift the 12hour offset following the definition of day_obs in https://sitcomtn-032.lsst.io/\n",
    "    # Drop the hours, minutes, seconds to get the ISO fromatted day_obs\n",
    "    day_obs = (Time.now() - TimeDelta(1.5, format='jd')).iso[:10]\n",
    "\n",
    "else:\n",
    "    # test that day_obs is a proper day_obs\n",
    "    try:\n",
    "        test_day_obs = Time(f\"{day_obs}T12:00:00\", format='isot', scale='utc')\n",
    "    except ValueError:\n",
    "        msg = \"day_obs should be a date formatted as YYYY-MM-DD\"\n",
    "        raise ValueError(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e9430-0899-4218-896b-d6083667d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "minutes_to_days = 1./60/24\n",
    "seconds_to_days = 1./60/60/24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0273835-d52b-4fb2-82d9-966e8612e57a",
   "metadata": {},
   "source": [
    "<a id=\"Almanac\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e75664-53f5-4748-83d4-479c1697db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Almanac ## \n",
    "\n",
    "display(Markdown(f\"## Almanac information for dayobs {day_obs}\"))\n",
    "site = Site('LSST')\n",
    "almanac = Almanac()\n",
    "night_events = almanac.get_sunset_info(evening_date=day_obs, longitude=site.longitude_rad)\n",
    "civil_sunset = Time(night_events['sunset'], format='mjd', scale='utc') \n",
    "sunset = Time(night_events['sun_n12_setting'], format='mjd', scale='utc') \n",
    "sunrise = Time(night_events['sun_n12_rising'], format='mjd', scale='utc')\n",
    "night_length = sunrise.mjd - sunset.mjd\n",
    "\n",
    "display(Markdown(f\"12-deg sunset at {sunset.to_datetime(timezone=tz_utc).strftime('%x %X')} UTC  -- {sunset.to_datetime(timezone=tz).strftime('%x %X')} {timezone}\"))\n",
    "display(Markdown(f\"12-deg sunrise at {sunrise.to_datetime(timezone=tz_utc).strftime('%x %X')} UTC --  {sunrise.to_datetime(timezone=tz).strftime('%x %X')} {timezone}\"))\n",
    "display(Markdown(f\"allowing for a night of {night_length * 24 :.2f} hours\"))\n",
    "moon_phase = almanac.get_sun_moon_positions(sunset.mjd)['moon_phase']\n",
    "if not np.isnan(night_events['moonrise']):\n",
    "    moonrise = Time(night_events['moonrise'], format='mjd', scale='utc')\n",
    "    display(Markdown(f\"Moonrise is at {moonrise.to_datetime(timezone=tz_utc).strftime('%x %X')} UTC -- {moonrise.to_datetime(timezone=tz).strftime('%x %X')} {timezone}\"))\n",
    "if not np.isnan(night_events['moonset']):\n",
    "    moonset = Time(night_events['moonset'], format='mjd', scale='utc')\n",
    "    display(Markdown(f\"Moonset at {moonset.to_datetime(timezone=tz_utc).strftime('%x %X')} UTC -- {moonset.to_datetime(timezone=tz).strftime('%x %X')} {timezone}\"))\n",
    "display(Markdown(f\"Moon phase is {moon_phase :.1f} (0=new, 100=full).\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe25532-a602-4ab4-95a1-575bef3dc455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report time notebook was run, which is likely useful if running notebook in the middle or start of the night\n",
    "current_time = Time.now()\n",
    "display(Markdown(f\"Time of notebook execution: {current_time.isot}\"))\n",
    "if current_time > sunrise:\n",
    "    display(Markdown(\"Night is complete.\"))\n",
    "elif current_time < sunrise and current_time > civil_sunset:\n",
    "    display(Markdown(\"Night is in progress.\"))\n",
    "elif current_time < civil_sunset:\n",
    "    display(Markdown(\"Night not yet started.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e60d9-302e-4f41-8f32-f5f9b4511c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "efd_client = EfdClient(efd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8dd651-6b62-4eba-b6be-a5ce4ebea985",
   "metadata": {},
   "source": [
    "<a id=\"EFD_configuration\"></a>\n",
    "\n",
    "## EFD Configuration Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43f4a7-b2fb-41fa-8f33-b9d760d7dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scheduler could be set up before sunset.\n",
    "early_setup = sunset - TimeDelta(6*60*60, format='sec')\n",
    "\n",
    "#display(Markdown(f\"## EFD Information\")) \n",
    "\n",
    "# What versions of the Scheduler modules are being used\n",
    "display(Markdown(\"### Scheduler Versions\"))\n",
    "\n",
    "# topic = 'lsst.sal.Scheduler.logevent_softwareVersions'\n",
    "# fields = await efd_client.get_fields(topic)\n",
    "# fields = [f for f in fields if \"private\" not in f]\n",
    "# dd = await efd_client.select_time_series(topic, fields, early_setup, sunrise, index=salindex)\n",
    "# display(dd)\n",
    "\n",
    "topic = 'lsst.sal.Scheduler.logevent_dependenciesVersions'\n",
    "fields = await efd_client.get_fields(topic)\n",
    "fields = [f for f in fields if \"private\" not in f]\n",
    "dd = await efd_client.select_time_series(topic, fields, early_setup, sunrise, index=salindex)\n",
    "display(dd)\n",
    "\n",
    "# How is the FBS and Scheduler configured \n",
    "display(Markdown(\"### Configurations applied\"))\n",
    "topic = 'lsst.sal.Scheduler.logevent_configurationApplied'\n",
    "fields = await efd_client.get_fields(topic)\n",
    "dd = await efd_client.select_time_series(topic, fields, early_setup, sunrise, index=salindex)\n",
    "if len(dd) == 0:\n",
    "    print(f\"No scheduler configurations applied between {early_setup.iso} and {sunrise.iso}\")\n",
    "else:\n",
    "    for i, row in dd[['configurations', 'schemaVersion', 'url']].iterrows():\n",
    "        print(i, row.configurations)\n",
    "        print(i, row.schemaVersion, row.url)\n",
    "print()\n",
    "\n",
    "# What other BLOCKS have been requested in this night (outside the FBS)\n",
    "display(Markdown(\"### JSON BLOCKS\"))\n",
    "#topic = 'lsst.sal.Scheduler.command_addBlock'\n",
    "topic = 'lsst.sal.Scheduler.logevent_blockStatus'\n",
    "fields = await efd_client.get_fields(topic)\n",
    "fields = [f for f in fields if 'private' not in f]\n",
    "dd = await efd_client.select_time_series(topic, fields, early_setup, sunrise, index=salindex)\n",
    "if len(dd) == 0:\n",
    "    print(f\"No JSON BLOCKS added between {early_setup.iso} and {sunrise.iso}\")\n",
    "else:\n",
    "    grouped_dd = dd.groupby('id')[['id', 'definition', 'executionsCompleted', 'hash', 'salIndex']].agg('max')\n",
    "    grouped_dd_start = dd.groupby('id')[['id', 'definition', 'executionsCompleted', 'hash', 'salIndex']].agg('min')\n",
    "    grouped_dd['night_executions']  = grouped_dd['executionsCompleted'] - grouped_dd_start['executionsCompleted']\n",
    "    display(grouped_dd[['id', 'definition', 'hash', 'executionsCompleted', 'night_executions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c868a9b8-bedf-40ac-9ec8-e83c48495ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might work .. to help translate test block numbers above into more meaningful programs\n",
    "jira_base_url = \"https://rubinobs.atlassian.net/projects/BLOCK?selectedItem=com.atlassian.plugins.atlassian-connect-plugin:com.kanoah.test-manager__main-project-page#!/v2/testCase/\"\n",
    "with open(\"/home/l/lynnej/.zephyr_token\", \"r\") as f:\n",
    "    zephyr_token = f.read().rstrip(\"\\n\")\n",
    "zephyr_url = \"https://api.zephyrscale.smartbear.com/v2/testcases/\"\n",
    "headers = {\"Accept\": \"application/json\",\n",
    "           \"Authorization\": f\"Bearer {zephyr_token}\",\n",
    "           \"Content-Type\": \"application/json\"} \n",
    "\n",
    "if len(dd) > 0:\n",
    "    for block in grouped_dd.id:\n",
    "        if block is not None and block.startswith(\"BLOCK-T\"):\n",
    "            response = requests.get(url=zephyr_url + block, headers=headers)\n",
    "            # It's possible that BLOCKS will be run that don't have a corresponding name in JIRA\n",
    "            if response.status_code == 200:\n",
    "                test_name = response.json()['name']\n",
    "                jira_url = jira_base_url + block\n",
    "                display(Markdown(f\"[{block}]({jira_url}) - {test_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87986df-81f5-42bf-b75f-c45b09a9c2d7",
   "metadata": {},
   "source": [
    "<a id=\"EFD_targets\"></a>\n",
    "\n",
    "## EFD Targets and Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfb48b5-6645-447c-a714-ead55c6ba536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch requested targets\n",
    "topic = 'lsst.sal.Scheduler.logevent_target'\n",
    "all_fields = await efd_client.get_fields(topic)\n",
    "#print(all_fields)\n",
    "targets = await efd_client.select_time_series(topic, all_fields, sunset, sunrise, index=salindex)\n",
    "\n",
    "def demangle_note_old(x):    \n",
    "    # remove _expnum\n",
    "    x.target = copy.deepcopy(x.note)\n",
    "    if \"IM\" in x.note:\n",
    "        x.note = x.note.split(\":\")[1].split(\"_\")[0]\n",
    "    if 'spec' in x.note:\n",
    "        x.note = 'HD' + x.note.split('HD')[-1]\n",
    "    return x\n",
    "\n",
    "# New note name -- still only note \n",
    "# But now note doubles for block + target name\n",
    "def demangle_note(x):    \n",
    "    vals = x.note.split(\":\")\n",
    "    block = x.note.split(\":\")[0]\n",
    "    if len(vals) > 1:\n",
    "        target_name = x.note.split(':')[1]\n",
    "        if \"HD\" in target_name: \n",
    "            # spec target\n",
    "            pass\n",
    "        elif \"_\" in target_name: \n",
    "            # most likely image target with tiles\n",
    "            # for compactness, remove tile\n",
    "            target_name = target_name.split(\"_\")[0]\n",
    "        x.science_program = block\n",
    "        x.target_name = target_name\n",
    "        x.note = target_name\n",
    "    else:\n",
    "        x.science_program = block\n",
    "        x.target_name = block\n",
    "        x.note = block\n",
    "    if x.note == \"BLOCK-305\":\n",
    "        x.note = 'cwfs'\n",
    "    return x\n",
    "\n",
    "if len(targets) == 0:\n",
    "    display(Markdown(f\"On night {day_obs} {telescope} recorded {len(targets)} target log events.\"))\n",
    "    display(Markdown(f\"This was for times between {sunset.iso}, {sunrise.iso}\"))\n",
    "\n",
    "    # debug help -- \n",
    "    targets = await efd_client.select_top_n(topic, all_fields, 2, index=salindex)\n",
    "    display(Markdown(\"The most recent targets were recorded were:\"))\n",
    "    display(targets[['SchedulerID', 'airmass', 'ra', 'decl', 'skyAngle', 'exposureTimes0', 'skyBrightness', 'slewTime', 'note', 'salIndex']])\n",
    "    targets = []\n",
    "\n",
    "else:\n",
    "\n",
    "    # target timestamp in EFD = time that the target is pushed to scriptqueue\n",
    "    # this should be at the start of the previous observation\n",
    "    targets[\"target_time\"] = targets.index.copy()\n",
    "    targets[\"target_mjd\"] = Time(targets.index).mjd\n",
    "    # estimate what time this target should be observed (start of observation)\n",
    "    # == target_time + previous exposure time (?) + slew time\n",
    "    cols = [c for c in targets if 'exposureTimes' in c]\n",
    "    targets['total_exptime'] = targets[cols].sum(axis=1)\n",
    "    \n",
    "    # Sometimes there are targets which don't correspond to a real target\n",
    "    # These seem to be triggered every time the Scheduler comes to ENABLED\n",
    "    targets = targets.query('total_exptime > 0')\n",
    "    \n",
    "    previous_exposure_time = np.concatenate([np.array([0]), targets['total_exptime'][:-1]])\n",
    "    # The target is supposed to be issued to the EFD when it hits the top of the scriptqueue \n",
    "    # which is supposed to be when the previous observation starts ... however\n",
    "    # an observation having many scripts may mean it doesn't hit the top of the scriptqueue then.\n",
    "    # (so this is probably more like a range of target.mjd + slewtime --- target + slewtime + previous exposure\n",
    "    targets[\"previous_exptime\"] = previous_exposure_time\n",
    "    targets[\"target_obsmjd\"] = Time(targets.index).mjd  + (targets['slewTime'])*seconds_to_days \n",
    "\n",
    "    # When estimating obsmjd expected from target -- include the previous\n",
    "    # exposure time or not? (in theory, should. in practice, queue is busy so not including it can be helpful).\n",
    "    include_prev_exptime = True\n",
    "\n",
    "    if include_prev_exptime:\n",
    "        targets[\"target_obsmjd\"] += (targets[\"previous_exptime\"])*seconds_to_days\n",
    "    \n",
    "    # demangle the note \n",
    "    targets['orig_note'] = targets.note.copy()\n",
    "    targets['target_name'] = ''\n",
    "    targets['block'] = ''\n",
    "    targets = targets.apply(demangle_note, axis=1)\n",
    "    \n",
    "    targets = targets.sort_values(by='target_obsmjd')\n",
    "    targets['target_id'] = np.arange(0, len(targets), 1)\n",
    "    targets = targets.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    targets.reset_index()\n",
    "    \n",
    "    display(Markdown(f\"On night {day_obs} {telescope} recorded {len(targets)} `target` events for \\n{targets.target_name.unique()}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866dc7ca-4e42-4a70-bb92-b5c875634fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations are the completed observation script .. to be compared with visits from the consdb\n",
    "\n",
    "topic = 'lsst.sal.Scheduler.logevent_observation'\n",
    "all_fields = await efd_client.get_fields(topic)\n",
    "obs = await efd_client.select_time_series(topic, all_fields, sunset, sunrise, index=salindex)\n",
    "\n",
    "if len(obs) == 0:\n",
    "    display(Markdown(f\"On night {day_obs} {telescope} recorded {len(obs)} observation log events.\"))\n",
    "\n",
    "else:\n",
    "    # timestamp is the time of successful end of observation/JSON block\n",
    "    obs['obs_time'] = obs.index.copy()\n",
    "    obs['obs_mjd'] = Time(obs.index).mjd \n",
    "\n",
    "    # obs time - exposure time should be start of observation (if exposure time is correct)\n",
    "    # Looking at values recorded, exptime * nexps is not total exposure time (exptime alone is)\n",
    "    obs['obs_obsmjd'] = Time(obs.index).mjd - (obs['exptime'] * seconds_to_days)\n",
    "    \n",
    "    obs = obs.sort_values(by='obs_obsmjd')\n",
    "    obs['obs_id'] = np.arange(0, len(obs), 1)\n",
    "    obs = obs.reset_index(drop=True)\n",
    "    \n",
    "    display(Markdown(f\"On night {day_obs} {telescope} recorded {len(obs)} `observation` events.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4e886-e93c-47d9-b724-d96a77ac52f2",
   "metadata": {},
   "source": [
    "### Matching `targets` to `observations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea280c-c029-4689-8f2f-70c26f503a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_obs_and_targets(obs, targets):\n",
    "    # This should be carried in the efd information - observation has a field for targetId\n",
    "    if len(targets) == 0:\n",
    "        print(\"No targets\")\n",
    "        return obs, targets\n",
    "    if len(obs) == 0:\n",
    "        print(\"No observations\")\n",
    "        targets['obs_id'] = -1\n",
    "        return obs, targets\n",
    "        \n",
    "    # Check targets -> observations\n",
    "    target_to_obs_match = np.zeros(len(targets), int) - 1\n",
    "    obs_to_target_match = np.zeros(len(obs), int) - 1\n",
    "    \n",
    "    print(\"Matching observations against targets\")\n",
    "    count = 0\n",
    "\n",
    "    # I'm finding that the first target can be very much delayed\n",
    "    # so let's treat that one separately\n",
    "    \n",
    "    for i, (ri, t) in enumerate(targets.iterrows()):\n",
    "        # Match obs_mjd, ra/dec/filter from target+observation\n",
    "        # the target.obs_mjd may overestimate actual mjd by ~ previous exposure\n",
    "        # if slewtime is inaccurate, target_obs.mjd may be inaccurate\n",
    "        # if visit includes JSON BLOCK with many steps, could be delays \n",
    "        # so that obs_obsmjd is later than reality\n",
    "        # some targets may never get completed observations\n",
    "        slew_error = 3.0 * minutes_to_days\n",
    "        target_error = t.previous_exptime * seconds_to_days\n",
    "        random_error = 14 * minutes_to_days\n",
    "        # don't match to a repeat of the same target much later\n",
    "        X = 30 * minutes_to_days\n",
    "        # CWFS target matching should try to use nearest time rather than ra/dec ? \n",
    "        if t.note == 'BLOCK-305':\n",
    "            delta_t = np.abs(t.target_obsmjd - obs.obs_obsmjd)\n",
    "            match = np.where((delta_t - delta_t.min() < 1e-2) # closest in time\n",
    "                             & ((obs.obs_obsmjd - t.target_obsmjd) <= X) # not more than X\n",
    "                             & (t['filter'] == obs['filter'])\n",
    "                             & (t['total_exptime'] == obs['exptime'])\n",
    "                             & (obs_to_target_match == -1))[0]\n",
    "        else:\n",
    "            match = np.where((np.abs(t.target_obsmjd - obs.obs_obsmjd) < target_error + slew_error + random_error) # close in time\n",
    "                             & ((obs.obs_obsmjd - t.target_obsmjd) <= X) # not more than X\n",
    "                             & (t.ra == obs.ra) \n",
    "                             & (t.decl == obs.decl) \n",
    "                             & (t['filter'] == obs['filter'])\n",
    "                             & (obs_to_target_match == -1))[0]\n",
    "\n",
    "        if len(match) == 0:\n",
    "            print(f'no obs match for target {t.target_id} {t.note}')\n",
    "            count += 1\n",
    "            target_to_obs_match[i] = -1\n",
    "        else:\n",
    "            # Consider target to have found the best match \n",
    "            # in the first (soonest) of 'match'\n",
    "            idx = match[0]\n",
    "            target_to_obs_match[i] = obs.iloc[idx]['obs_id']\n",
    "            # And consider observation to have found a match\n",
    "            obs_to_target_match[idx] = t['target_id']\n",
    "\n",
    "    obs['target_id'] = np.array(obs_to_target_match)\n",
    "    targets['obs_id'] = np.array(target_to_obs_match)\n",
    "    \n",
    "    print(f'failed to match {count} targets to observations')\n",
    "    print(f'compare to {len(targets) - len(obs)} expected difference')\n",
    "    \n",
    "    return obs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b43a-b9e0-41ef-aba2-892c6cfb33f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short columns helpful for cross-matching tests\n",
    "target_cols = ['ra', 'decl', 'skyAngle', 'filter', 'total_exptime', 'note', 'target_obsmjd']\n",
    "obs_cols  = ['ra', 'decl', 'rotSkyPos', 'filter', 'exptime', 'nexp', 'obs_obsmjd',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8df77e-41bc-4eca-beb3-d3b6acab871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, targets = match_obs_and_targets(obs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052ce25-8d0b-4762-9f15-5fde1cd2882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets which did not get observations\n",
    "if len(targets) > 0:\n",
    "    tt = targets.query('obs_id == -1')[['ra', 'decl', 'skyAngle', 'filter', 'total_exptime', 'note', 'orig_note', 'target_mjd', 'obs_id',]]\n",
    "    if len(tt) > 0:\n",
    "        display(Markdown(\"The following `targets` were not able to be linked with `observations`\"))\n",
    "        display(tt)\n",
    "    else:\n",
    "        display(Markdown(\"All logged `targets` were linked with `observations`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf951f2-f79a-44e1-8724-b214687270e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(obs) > 0:\n",
    "    oo = obs.query('target_id == -1')[['ra', 'decl', 'rotSkyPos', 'filter', 'exptime', 'obs_mjd', 'target_id']]\n",
    "    if len(oo) > 0:\n",
    "        display(Markdown(\"The following `observations` were unable to be matched with `targets` (this is unusual).\"))\n",
    "        display(oo)\n",
    "    else:\n",
    "        display(Markdown(\"All `observations` were linked with `targets`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1665de7-1903-4c50-867d-540b03b6a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join target and observations into one dataframe\n",
    "if len(obs) > 0:\n",
    "    # Join targets + obs\n",
    "    jj = obs.query('target_id > -1').join(targets, on='target_id', lsuffix='_o', rsuffix='_t')\n",
    "    jj = jj.sort_values(by='obs_obsmjd')\n",
    "    \n",
    "    jj['delta_obs'] = np.concatenate([np.array([0]), np.diff(jj['obs_obsmjd'])/minutes_to_days])  # minutes -- should be close to expected overhead\n",
    "    jj['delta_request'] = (jj['obs_obsmjd'] - jj['target_obsmjd']) / minutes_to_days  # minutes  -- should be close to 0\n",
    "    jj['expected_gap'] = (jj['previous_exptime']+ jj['slewTime'])/60  # minutes - expected overhead from models\n",
    "    anomalous_overhead = jj['delta_obs'].values - jj['expected_gap'].values\n",
    "    anomalous_overhead[0] = 0\n",
    "    jj['anomalous_overhead'] = anomalous_overhead\n",
    "else:\n",
    "    jj = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99de7cd-c46e-4ddb-9cf5-344ac9d68546",
   "metadata": {},
   "source": [
    "<a id=\"ConsDb_visits\"></a>\n",
    "\n",
    "## ConsDb Visit Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610d121-b75a-456c-b642-23aa29dd0d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add consdb\n",
    "\n",
    "day_obs_int = int(day_obs.replace('-', ''))\n",
    "\n",
    "visit_query = f'''\n",
    "    SELECT * FROM cdb_{instrument}.visit1\n",
    "     where day_obs = {day_obs_int}\n",
    "'''\n",
    "\n",
    "quicklook_query = f'''\n",
    "    SELECT q.*  FROM cdb_{instrument}.visit1_quicklook as q,\n",
    "    cdb_{instrument}.visit1 as v\n",
    "     WHERE v.day_obs = {day_obs_int} and q.visit_id = v.visit_id\n",
    "'''\n",
    "\n",
    "# Use the ConsDB Client, and add a couple of tries \n",
    "consdb = ConsDbClient()\n",
    "\n",
    "# Ugh, wrap the whole thing again in case the consdb is just down\n",
    "try: \n",
    "    try:\n",
    "        visits = consdb.query(visit_query).to_pandas()\n",
    "    except requests.HTTPError or requests.JSONDecodeError:\n",
    "        # Try twice\n",
    "        visits = consdb.query(visit_query).to_pandas()\n",
    "    \n",
    "    quicklook = consdb.query(quicklook_query).to_pandas()\n",
    "except requests.HTTPError:\n",
    "    display(Markdown(\"ConsDB seems to be unreachable\"))\n",
    "    visits = []\n",
    "    quicklook = []\n",
    "\n",
    "if len(visits) > 0:\n",
    "    display(Markdown(f\"Retrieved {len(visits)} visits from consdb\"))\n",
    "    obj_visits = visits.query('img_type == \"OBJECT\"')\n",
    "    display(Markdown(f\"{len(obj_visits)} of these are `OBJECT` images\"))\n",
    "\n",
    "if len(quicklook) > 0:\n",
    "    visits = visits.join(quicklook, on='visit_id', lsuffix='', rsuffix='_q')\n",
    "    visits = visits.copy()\n",
    "    display(Markdown(f\"And added quicklook stats\"))\n",
    "\n",
    "if len(visits) == 0:\n",
    "    display(Markdown(f\"No visits for {telescope} on {day_obs} retrieved from consdb\"))\n",
    "\n",
    "# Patch science_program if was None\n",
    "values = (dict([[e,\"\"] for e in ['science_program','target_name', 'observation_reason']]))\n",
    "visits.fillna(value=values, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f668a-ec1b-4fc1-9b5a-4ca76845c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "short_cols = ['seq_num', 'obs_start_mjd', 'obs_end_mjd', 'exp_time', 'shut_time', 'dark_time', 's_ra', 's_dec', 'band', 'airmass', \n",
    "              'img_type', 'target_name', 'science_program', 'observation_reason', 'dimm_seeing']\n",
    "if len(visits)>0 and verbose:\n",
    "    display(visits[short_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286ec95-7e77-4deb-a7d3-036c3324da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a 'note' to match visits with the target/observation colors\n",
    "def construct_note_old(x):\n",
    "    if x.science_program == 'cwfs' or x.science_program == 'cwfs-focus-sweep':\n",
    "        note = 'cwfs'\n",
    "    elif x.science_program == \"BLOCK-295\":\n",
    "        note = 'calibrations'\n",
    "    elif x.target_name == 'FlatField position':\n",
    "        note = 'calibrations'\n",
    "    elif x.science_program == 'AUXTEL_PHOTO_IMAGING':\n",
    "        note = x.target_name.split('_')[0]\n",
    "    elif x.science_program == 'spec-survey':\n",
    "        note = x.target_name\n",
    "    else:\n",
    "        note = 'unknown'\n",
    "    return note\n",
    "\n",
    "def construct_note(x):\n",
    "    if x.science_program  == \"BLOCK-305\":\n",
    "        note = 'cwfs'\n",
    "    else:\n",
    "        if len(x.science_program) > 0: \n",
    "            note = x.science_program\n",
    "        elif len(x.target_name) > 0: \n",
    "            # Then try to split off tile name\n",
    "            note = x.target_name\n",
    "        else:\n",
    "            note = x.observation_reason\n",
    "        if note is None:\n",
    "            note = ''\n",
    "    x.note = note\n",
    "    return x\n",
    "\n",
    "if len(visits)>0:\n",
    "    visits['note'] = ''\n",
    "    visits = visits.apply(construct_note, axis=1)\n",
    "    display(Markdown(f\"ConsDB constructed note names: {visits.note.unique()}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260540c0-4902-47b0-8b8b-73bba6b9b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link consdb visits with targets .. note that this can be many visits -> one target\n",
    "# Also, should match against targets and not observations, because there can be some visits from an incomplete target\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    # probably shouldn't entirely suppress this, but can't quite find PerformanceWarning source\n",
    "    warnings.simplefilter('ignore')\n",
    "    if len(visits) > 0:\n",
    "        visit_target_id = np.zeros(len(visits)) - 1\n",
    "        if len(targets) > 0:\n",
    "            for note in targets.note.unique():\n",
    "                vv = visits.query('note == @note and science_program != None')\n",
    "                if len(vv) == 0:\n",
    "                    continue\n",
    "                tt = targets.query('note == @note')\n",
    "                # Find sequential visits \n",
    "                gaps = [[s, e] for s, e, in zip(vv.index, vv.index[1:]) if s+1 < e]\n",
    "                left = [vv.index[0]] + [g[1] for g in gaps]\n",
    "                right = [g[0] for g in gaps] + [vv.index[-1]]\n",
    "                consecutive = list(zip(left, right))\n",
    "                for l, r in zip(left, right):\n",
    "                    # if there are repeated targets with the same note, resulting in\n",
    "                    # sequential visits to the same note ... this will match all \n",
    "                    # the visits to the first target in the series (unfortunately)\n",
    "                    delta_t = vv.loc[l]['obs_start_mjd'] - tt['target_mjd']\n",
    "                    idx = np.where(delta_t > 0, delta_t, np.inf).argmin()\n",
    "                    visit_target_id[l:(r+1)] = tt.iloc[idx]['target_id']\n",
    "            \n",
    "        visits['target_id'] = visit_target_id\n",
    "\n",
    "    \n",
    "    display(Markdown(f\"Matched {len(visits.query('target_id > 1'))} `visits` to `targets`, out of a total of {len(visits)} visits.\"))\n",
    "    remainder = visits.query('target_id == -1')\n",
    "    display(Markdown(f\"The remaining {len(remainder)} visits were of type {remainder.img_type.unique()}\"))\n",
    "                     #f\" with science_programs {remainder.science_program.unique()}, observation_reason {remainder.observation_reason.unique()}.\"))\n",
    "\n",
    "# Matched!  .. could now compare time of actual visit with time of target and time of observation, try to sort out if there are timing issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de68f233-b582-48a6-ac81-1601562cb24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = None\n",
    "if len(visits) > 0:\n",
    "    groupcols = ['science_program', 'img_type', 'target_name', 'observation_reason', 'day_obs', 'visit_id'] \n",
    "    c = visits[groupcols].groupby(['science_program', 'img_type']).agg({'science_program' : ['first'],\n",
    "                                                                        'target_name' : ['unique'], \n",
    "                                                                        'observation_reason' : ['unique'],\n",
    "                                                                        'day_obs' : ['nunique'],\n",
    "                                                                        'visit_id' : ['first', 'last', 'count']})\n",
    "    display(Markdown(f\"ConsDB Visits\"))\n",
    "    with option_context('display.max_colwidth', None):\n",
    "        display(HTML(c.sort_values(by=('visit_id', 'first')).to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3350f31-ca7f-475c-80d2-042f8e399403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might work .. to help translate test block numbers above into more meaningful programs\n",
    "jira_base_url = \"https://rubinobs.atlassian.net/projects/BLOCK?selectedItem=com.atlassian.plugins.atlassian-connect-plugin:com.kanoah.test-manager__main-project-page#!/v2/testCase/\"\n",
    "with open(\"/home/l/lynnej/.zephyr_token\", \"r\") as f:\n",
    "    zephyr_token = f.read().rstrip(\"\\n\")\n",
    "zephyr_url = \"https://api.zephyrscale.smartbear.com/v2/testcases/\"\n",
    "headers = {\"Accept\": \"application/json\",\n",
    "           \"Authorization\": f\"Bearer {zephyr_token}\",\n",
    "           \"Content-Type\": \"application/json\"} \n",
    "\n",
    "if len(visits) > 0:\n",
    "    for science_program in visits.science_program.unique():\n",
    "        if science_program is not None and science_program.startswith(\"BLOCK-T\"):\n",
    "            response = requests.get(url=zephyr_url + science_program, headers=headers)\n",
    "            test_name = response.json()['name']\n",
    "            jira_url = jira_base_url + science_program\n",
    "            display(Markdown(f\"[{science_program}]({jira_url}) - {test_name}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e3c4a-a54a-414a-b3f3-7912e37e1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(targets.target_id, targets.target_obsmjd, 'r.')\n",
    "#plt.plot(visits.query('target_id > -1').target_id, visits.query('target_id > -1').obs_start_mjd, 'k.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045c824-0359-454e-ac9d-4bed914cc192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a neat trick to dynamically create a download link .. but probably not necessary here \n",
    "\n",
    "# import base64\n",
    "\n",
    "# def create_download_link( df, title = \"Download CSV file\", filename = \"data.csv\"):\n",
    "#     csv = df.to_csv()\n",
    "#     b64 = base64.b64encode(csv.encode())\n",
    "#     payload = b64.decode()\n",
    "#     html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n",
    "#     html = html.format(payload=payload,title=title,filename=filename)\n",
    "#     return HTML(html)\n",
    "\n",
    "# display(create_download_link(targets, title=f\"Download linked targets CSV ({day_obs})\"))\n",
    "# display(create_download_link(obs, title=f\"Download linked observations CSV ({day_obs})\"))\n",
    "# display(create_download_link(visits, title=f\"Download linked visits CSV ({day_obs})\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b422b44-bdef-4e2f-81ea-569365a07c24",
   "metadata": {},
   "source": [
    "<a id=\"Overheads\"></a>\n",
    "\n",
    "## Overheads and Issues\n",
    "\n",
    "Comparing the expected overhead from the predicted slewtime and exposure time to the actual time recorded between observation events (where matched to targets). \n",
    "This can be used to identify longer-than-expected overheads between visits. \n",
    "Currently, this doesn't use the Visit information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2e407e-08c6-4264-838b-18fa19901b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overhead_error = 3\n",
    "typical_anomalous_overhead = 0\n",
    "\n",
    "if len(jj) > 0:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    _ = ax[0].hist(jj['delta_obs'], bins=np.arange(-2, 25, 0.5), alpha=0.6, histtype='bar', label='delta observation obsmjd')\n",
    "    _ = ax[0].hist(jj['expected_gap'], bins=np.arange(-2, 25, 0.5), alpha=0.6, histtype='bar', label='predicted gaps between obs')\n",
    "    ax[0].legend(loc=(0.45, 0.85))\n",
    "    ax[0].set_xlabel(\"Minutes\", fontsize='large')\n",
    "\n",
    "    _ = ax[1].hist(jj['anomalous_overhead'], bins=50, histtype='bar', label=\"(delta_obs) - (pred_gap)\")\n",
    "    ax[1].axvline(overhead_error, color='r', linestyle=':')\n",
    "    ax[1].legend(loc=(0.58, 0.83))\n",
    "    ax[1].set_xlabel(\"Anomalous Overhead (minutes)\", fontsize='large')\n",
    "\n",
    "    typical_anomalous_overhead = np.median(jj.query('anomalous_overhead < @overhead_error')['anomalous_overhead'])\n",
    "    display(Markdown(f\"Median value of anomalous overhead, not including values beyond dashed line:  {typical_anomalous_overhead.round(2)} minutes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c3fd5-2621-400a-a148-938bf6fa2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where did the time between observations just take longer than expected \n",
    "# delta obs compared to (slewTime + exptime) BUT sheduler target estimate was correct\n",
    "\n",
    "breaks = []\n",
    "if len(jj)>0: \n",
    "    overheads = np.where(jj.anomalous_overhead > overhead_error)[0]\n",
    "    \n",
    "    for b in overheads:\n",
    "        b_start = jj.iloc[b-1]['obs_obsmjd'] + jj.iloc[b-1]['exptime']/60/60/24\n",
    "        b_end = jj.iloc[b]['obs_obsmjd'] #- jj.iloc[b]['slewTime']/60/60/24\n",
    "        # look for cwfs sweeps around the break, as they seem to mess with timings\n",
    "        b_min = np.max([b-1, 0])\n",
    "        b_max = np.min([b+1, len(jj)-1])\n",
    "        check_for_cwfs = jj.iloc[b_min:b_max+1].query(\"note == 'cwfs'\")\n",
    "        if len(check_for_cwfs) > 0 and (b_end - b_start) < 10 * minutes_to_days:\n",
    "            # cwfs sweeps have variable timing and can disturb the expected timings\n",
    "            # but also, if we did have a long break, then count these anyway.\n",
    "            pass\n",
    "        else:\n",
    "            breaks.append([b_start, b_end])\n",
    "    \n",
    "    print(f\"There are {len(overheads)} points where the time between observations doesn't match predicted overheads within {overhead_error} minutes.\",\n",
    "           f\" CWFS visits are indicated in {len(overheads) - len(breaks)} of these breaks, and have a variable effect on the overhead so will be discounted.\")\n",
    "    print()\n",
    "    \n",
    "    display(Markdown(\"Details of unexpected deltas in targets-observations\"))\n",
    "    for o in overheads:\n",
    "        display(jj[['target_id', 'obs_id_t', 'target_name', 'delta_obs', 'expected_gap', 'obs_obsmjd', 'target_obsmjd']][o-1:o+2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e70331-15e8-4583-8926-d5b2fb9c5aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(jj) > 0:\n",
    "    plt.figure()\n",
    "    \n",
    "    eps = 1\n",
    "    for note in jj.note.unique():\n",
    "        j = jj.query('note == @note')\n",
    "        plt.plot(j.delta_obs, j.expected_gap, '.', label=note)\n",
    "    j = jj.iloc[overheads]\n",
    "    plt.plot(j.delta_obs, j.expected_gap, 'o',\n",
    "             markersize=10, color='k', markerfacecolor='none', label='Big Overhead')\n",
    "    plt.legend(loc=(1.01, 0.5))\n",
    "    for exptime in jj.exptime.unique():\n",
    "        plt.axhline(exptime/60, color='gray', linestyle=':', alpha=0.6)\n",
    "    \n",
    "    x = np.arange(0, 60)\n",
    "    plt.plot(x, x, 'r:')\n",
    "    plt.fill_between(x, x-overhead_error, x+overhead_error,  color='r', alpha=0.1)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.xlim(0, np.max(jj.delta_obs) + eps)\n",
    "    plt.ylim(0, np.max(jj.expected_gap) + eps/2)\n",
    "    \n",
    "    plt.xlabel(\"DeltaT between start of observations (minutes)\")\n",
    "    plt.ylabel(\"Expected gap (exptime + slew) (minutes)\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ed291-4f69-4f37-8e81-082ba9f8d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify any sequences of incomplete targets, to color-code later\n",
    "breaks_targets = []\n",
    "if len(targets) > 0:\n",
    "    t = targets.query('obs_id == -1').index.values\n",
    "    gaps = [[s, e] for s, e, in zip(t, t[1:]) if s+1 < e]\n",
    "    left = [t[0]] + [g[1] for g in gaps]\n",
    "    right = [g[0] for g in gaps] + [t[-1]]\n",
    "    consecutive = list(zip(left, right))\n",
    "    # Trim off the end, as a likely single-missed target\n",
    "    if consecutive[-1][0] == consecutive[-1][1]:\n",
    "        consecutive = consecutive[:-1]\n",
    "    missing_break = 0\n",
    "    for c_visits in consecutive:\n",
    "        start_idx = c_visits[0]\n",
    "        end_idx = c_visits[1] + 1\n",
    "        if end_idx > start_idx + 1: \n",
    "            # Use time target recorded at as the start\n",
    "            b_start = targets.iloc[start_idx]['target_mjd']\n",
    "            # Use time target would have expected observation as end point\n",
    "            if end_idx > len(targets) - 1:\n",
    "                end_idx = end_idx - 1\n",
    "            b_end = targets.iloc[end_idx]['target_obsmjd']\n",
    "            breaks_targets.append([b_start, b_end])\n",
    "            missing_break += 1\n",
    "\n",
    "    if missing_break == 1:\n",
    "        display(Markdown(f\"There was {missing_break} period of consecutive incomplete `targets`. Could be due to weather or script failures.\"))\n",
    "    if missing_break > 1:\n",
    "        display(Markdown(f\"There were {missing_break} periods of consecutive incomplete `targets`. Could be due to weather or script failures.\"))\n",
    "    if missing_break == 0:\n",
    "        display(Markdown(\"There were no periods of consecutive incomplete `targets`.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ede609e-5c40-4ced-ae14-041fa6fe8dad",
   "metadata": {},
   "source": [
    "## Time Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e87400-c168-4498-afc6-e8b50d560a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try summing up time into different buckets, although it should be noted that this is still in need of more validation\n",
    "\n",
    "# First find when the ScriptQueue was running and enabled .. this could be slightly different than when Targets are sent.\n",
    "topic = 'lsst.sal.ScriptQueue.logevent_queue'\n",
    "fields = ['running', 'enabled', 'currentSalIndex', 'salIndices0', 'pastSalIndices0']\n",
    "dd = await efd_client.select_time_series(topic, fields, civil_sunset, sunrise)\n",
    "\n",
    "if len(dd) > 0:\n",
    "    # Parse out simonyi / auxtel / ocs entries from the scriptqueue\n",
    "    query = f\"pastSalIndices0.astype('str').str.startswith('{salindex}')\"\n",
    "    dd = dd.query(query)\n",
    "\n",
    "if len(dd) > 0:\n",
    "    dd['go'] = (dd['running'] & dd['enabled']).values\n",
    "    dd['go_prev'] = np.concatenate([np.array([False]), dd['go'].values[:-1]])\n",
    "    start = dd.query('(go != go_prev) and (go)').index.values\n",
    "    end = dd.query('(go != go_prev) and not (go)').index.values\n",
    "    if len(end) == len(start) - 1:\n",
    "        end = np.concatenate([end, dd.index[-1:].values])\n",
    "    start_queue = Time(start).mjd\n",
    "    end_queue = Time(end).mjd\n",
    "    time_queue_active = (end_queue - start_queue).sum()\n",
    "    sunset_to_queue = (start_queue[0] - sunset.mjd)\n",
    "else:\n",
    "    time_queue_active = 0\n",
    "    sunset_to_queue = night_length\n",
    "\n",
    "# Also look at when targets were being sent for this telescope\n",
    "if len(targets) > 0:\n",
    "    last_target = targets.iloc[-1]['target_mjd']\n",
    "    first_target = targets.iloc[0]['target_mjd']\n",
    "    time_with_targets = (last_target - first_target) \n",
    "    sunset_to_targets = (first_target - sunset.mjd)\n",
    "else:\n",
    "    time_with_targets = 0\n",
    "    sunset_to_targets = night_length\n",
    "\n",
    "# Let's assume we found the breaks accurately ... but also try not to double count\n",
    "tt = np.arange(sunset.mjd, sunrise.mjd, 1/60/24)\n",
    "val = np.zeros(len(tt))\n",
    "for b in breaks:\n",
    "    val = np.where((tt >= b[0]) & (tt <= b[1]), 1, val)\n",
    "for b in breaks_targets:\n",
    "    val = np.where((tt >= b[0]) & (tt <= b[1]), 1, val)\n",
    "time_in_breaks = val.sum() * minutes_to_days\n",
    "\n",
    "# Predicted overhead in slews + anomalous overhead\n",
    "if len(jj) > 0:\n",
    "    time_overhead = (jj['slewTime'].sum() + jj['anomalous_overhead'].sum()) * seconds_to_days\n",
    "else:\n",
    "    time_overhead = 0\n",
    "\n",
    "\n",
    "# Count up observing time per science_program during the night\n",
    "# Just roll up OBJECT, CWFS, ACQ and FOCUS .. \n",
    "night_visits = 0\n",
    "if len(visits) > 0:\n",
    "    visits_after_sunset = visits.query(\"obs_start_mjd > @sunset.mjd\")\n",
    "    night_visits = len(visits_after_sunset)\n",
    "    if night_visits > 0:\n",
    "        programs = visits_after_sunset.science_program.unique()\n",
    "        onsky_time = {}\n",
    "        for p in programs:\n",
    "            onsky_time[p] = visits_after_sunset.query('science_program == @p').exp_time.sum() * seconds_to_days\n",
    "        # Consolidate \"random\" programs .. while getting metadata sorted\n",
    "        onsky_time['Unknown'] = onsky_time['']\n",
    "        del onsky_time['']\n",
    "        sunset_to_visits = (visits_after_sunset.iloc[0].obs_start_mjd - sunset.mjd) \n",
    "        civilsunset_to_visits = (visits_after_sunset.iloc[0].obs_start_mjd - civil_sunset.mjd) \n",
    "if night_visits == 0:\n",
    "    sunset_to_visits = night_length\n",
    "    civilsunset_to_visits = night_length\n",
    "    onsky_time = {}\n",
    "\n",
    "dd = pd.DataFrame([night_length, time_queue_active, time_with_targets, \n",
    "                   sunset_to_queue, sunset_to_targets, sunset_to_visits, civilsunset_to_visits,\n",
    "                   time_in_breaks, time_overhead,], \n",
    "                  index=[\"Total night time\", \"Time ScriptQueue Active\", \"Time with Targets\",\n",
    "                         \"Sunset to Scriptqueue\", \"Sunset to Targets\", \"Sunset to Visits\", \"Civil Sunset to Visits\",\n",
    "                         \"Possible issue time\", \"Overheads\", \n",
    "                         ], \n",
    "                  columns=[day_obs])\n",
    "dd_obs = pd.DataFrame(onsky_time, index=[day_obs])\n",
    "# convert days (above) to hours\n",
    "dd = (dd * 24).T\n",
    "dd_obs = (dd_obs * 24)\n",
    "\n",
    "display(Markdown(\"Prototyping summary of time accounting, WIP\"))\n",
    "display(Markdown(\"Units: Hours. Sunset: -12 degree.\"))\n",
    "display(dd.round(2))\n",
    "display(Markdown(\"Exposure time per science_program\"))\n",
    "display(dd_obs.round(2))\n",
    "\n",
    "dd = dd.join(dd_obs)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 8))\n",
    "fig.tight_layout(pad=17)\n",
    "cols = list(onsky_time.keys()) #+ [\"Possible issue time\", \"Overheads\", ]\n",
    "plot_data = dd.loc[:, cols] / dd.iloc[0, 1]\n",
    "plot_data.plot(kind='bar', stacked=True, ax=ax[0],\n",
    "               ylim=[0, 1],\n",
    "               legend=False, ylabel='Fraction of Time with Active ScriptQueue').legend(loc=(1.01, 0.2))\n",
    "ax[0].grid(True, alpha=0.3)\n",
    "\n",
    "cols = list(onsky_time.keys()) + [\"Time with Targets\", \"Time ScriptQueue Active\", \"Total night time\"]\n",
    "plot_data = dd.loc[:, cols]\n",
    "plot_data.plot(kind='bar', stacked=False, ax=ax[-1],\n",
    "               legend=False, ylabel='Hours').legend(loc=(1.01, 0.2))\n",
    "ax[-1].grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6d6a6-55b3-4936-9b03-170335ebea1a",
   "metadata": {},
   "source": [
    "<a id=\"Summary_plot\"></a>\n",
    "\n",
    "## Summary Plot of FBS Targets, Observations, and ConsDB Visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89bf7e-ba97-4167-8f48-2999702d6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mjd_to_datetime(mjd, timezone=tz):\n",
    "    return Time(mjd, format='mjd', scale='utc').to_datetime(timezone=timezone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9cdb65-a38c-49f3-a86c-68427d475110",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1\n",
    "fig, ax = plt.subplots(figsize=(11, 7))\n",
    "ax_utc = ax.twiny()\n",
    "\n",
    "ax.set_title(f\"{telescope} DAYOBS {day_obs}\", pad=20)\n",
    "\n",
    "\n",
    "# Shade astronomical events\n",
    "ax.fill_between([mjd_to_datetime(night_events['sun_n12_setting']), \n",
    "                  mjd_to_datetime(night_events['sun_n18_setting'])],\n",
    "                 2.5, 0.0, color='lightgray', alpha=0.3)\n",
    "ax.fill_between([mjd_to_datetime(night_events['sunset']), \n",
    "                 mjd_to_datetime(night_events['sun_n12_setting'])], \n",
    "                  2.5, 0.0, color='gray', alpha=0.3)\n",
    "ax.fill_between([mjd_to_datetime(night_events['sun_n18_rising']), \n",
    "                 mjd_to_datetime(night_events['sun_n12_rising'])],\n",
    "                 2.5, 0.0, color='lightgray', alpha=0.3)\n",
    "ax.fill_between([mjd_to_datetime(night_events['sun_n12_rising']), \n",
    "                 mjd_to_datetime(night_events['sunrise'])],\n",
    "                 2.5, 0.0, color='gray', alpha=0.3)\n",
    "\n",
    "if not np.isnan(night_events['moonrise']):\n",
    "    ax.axvline(mjd_to_datetime(night_events['moonrise']), linestyle=':', color='blue', alpha=0.3)\n",
    "if not np.isnan(night_events['moonset']):\n",
    "    ax.axvline(mjd_to_datetime(night_events['moonset']), linestyle=':', color='red', alpha=0.3)\n",
    "\n",
    "plot_fbs = True\n",
    "\n",
    "plot_visits = True\n",
    "if len(visits) == 0:\n",
    "    plot_visits = False\n",
    "\n",
    "# Assign distinct target sets with different colors\n",
    "if len(jj) > 0:\n",
    "    notes = jj.note.unique()\n",
    "    note_colors = {}\n",
    "    for i, n in enumerate(notes):\n",
    "        note_colors[n] = cc.glasbey[i]\n",
    "elif len(visits) > 0:\n",
    "    notes = visits.note.unique()\n",
    "    note_colors = {}\n",
    "    for i, n in enumerate(notes):\n",
    "        note_colors[n] = cc.glasbey[i]\n",
    "else: # no completed observations, no visits, but targets\n",
    "    notes = targets.note.unique()\n",
    "    note_colors = {}\n",
    "    for i, n in enumerate(notes):\n",
    "        note_colors[n] = cc.glasbey[i]\n",
    "\n",
    "if plot_fbs and len(jj) > 0:\n",
    "    # Plot the successfully recorded 'observation' scripts\n",
    "    for note in jj.note.unique():\n",
    "        j = jj.query('note == @note')\n",
    "        ax.plot(mjd_to_datetime(j.obs_obsmjd), j.airmass, '.', \n",
    "                markersize=9,  color=note_colors[note], label=f\"Target+Obs {note}\")\n",
    "\n",
    "    # Shade breaks in observation events\n",
    "    if len(breaks) > 0:\n",
    "        for break_count, b in enumerate(breaks):\n",
    "            ax.fill_between(mjd_to_datetime(b), 2.5, 0.0, color='pink', alpha=0.3)\n",
    "            ax.text(mjd_to_datetime(b[0]), 0.97, f\"D-{break_count}\")\n",
    "\n",
    "    # Plot the observations that come after what looks like big overheads\n",
    "    #j = jj.iloc[overheads]\n",
    "    #plt.plot(j.obs_obsmjd, j.airmass, 'o', \n",
    "    #         markersize=10, color='k', markerfacecolor='none', label='Big Overheads')\n",
    "\n",
    "if plot_fbs and len(targets) > 0:\n",
    "    # Plot the targets which did not get 'observations' recorded\n",
    "    incomplete_targets = targets.query('obs_id == -1')\n",
    "    for note in incomplete_targets.note.unique():\n",
    "        try:\n",
    "            color = note_colors[note]\n",
    "        except KeyError:\n",
    "            color = cc.glasbey[i]\n",
    "            i -= 1\n",
    "        t = incomplete_targets.query('note == @note')\n",
    "        ax.plot(mjd_to_datetime(t.target_obsmjd), \n",
    "                 t.airmass, '+', markersize=11,\n",
    "                 label=f'TargetOnly {note}', color=color, zorder=1)\n",
    "\n",
    "    # Shade incomplete target streaks\n",
    "    if len(breaks_targets) > 0:\n",
    "        for break_count, b in enumerate(breaks_targets):\n",
    "            ax.fill_between(mjd_to_datetime(b), 2.5, 0.0, color='lightblue', alpha=0.3)\n",
    "            ax.text(mjd_to_datetime(b[0]), 0.94, f\"T-{break_count}\")\n",
    "\n",
    "\n",
    "    # Shade time before first targets \n",
    "    # ax.fill_between([mjd_to_datetime(sunset.mjd), mjd_to_datetime(targets.target_mjd[0])],\n",
    "    #                2.5, 0.0, color='darkorange', alpha=0.1)\n",
    "\n",
    "\n",
    "i = len(cc.glasbey) - 1\n",
    "if plot_visits: \n",
    "    # Plot the visits recorded in the consdb \n",
    "    obs_visits = visits.query('img_type == \"OBJECT\"')\n",
    "    ac_visits = visits.query('img_type == \"ACQ\"')\n",
    "    focus_visits = visits.query('img_type == \"FOCUS\"')\n",
    "    cwfs_visits = visits.query('img_type == \"CWFS\"')\n",
    "    for note in visits.note.unique():\n",
    "        try:\n",
    "            color = note_colors[note]\n",
    "        except KeyError:\n",
    "            color = cc.glasbey[i]\n",
    "            i -= 1\n",
    "        # Label the first (in order) of these we find\n",
    "        label = f'ConsDb {note}'\n",
    "        v = obs_visits.query('note == @note')\n",
    "        if len(v) > 0:\n",
    "            ax.plot(mjd_to_datetime(v.obs_start_mjd), v.airmass, 'o', \n",
    "                     markersize=7, markerfacecolor='none', alpha=0.5,\n",
    "                     label=label, color=color, zorder=3)\n",
    "            label = None\n",
    "        v = cwfs_visits.query('note == @note')\n",
    "        if len(v) > 0:\n",
    "            ax.plot(mjd_to_datetime(v.obs_start_mjd), v.airmass, 'H', \n",
    "                     markersize=8, markerfacecolor='none', alpha=0.5,\n",
    "                     label=label, color=color, zorder=3)\n",
    "            label = None\n",
    "        v = ac_visits.query('note == @note')\n",
    "        if len(v) > 0:\n",
    "            ax.plot(mjd_to_datetime(v.obs_start_mjd), v.airmass, '*', \n",
    "                     markersize=9, markerfacecolor='none', alpha=0.5,\n",
    "                     label=label, color=color, zorder=3)\n",
    "            label = None\n",
    "        v = focus_visits.query('note == @note')\n",
    "        if len(v) > 0:\n",
    "            ax.plot(mjd_to_datetime(v.obs_start_mjd), v.airmass, 'D', \n",
    "                     markersize=5, markerfacecolor='none', alpha=0.5,\n",
    "                     label=label, color=color, zorder=3)\n",
    "            label = None\n",
    "\n",
    "ax.legend(loc=(1.01, 0.0), ncol=2)\n",
    "\n",
    "x0 = night_events['sunset']+30/60/24\n",
    "\n",
    "ax.set_xlim(mjd_to_datetime(night_events['sunset']+30/60/24), \n",
    "         mjd_to_datetime(night_events['sunrise']-30/60/24))\n",
    "ax_utc.set_xlim(mjd_to_datetime(night_events['sunset']+30/60/24, timezone=tz_utc), \n",
    "         mjd_to_datetime(night_events['sunrise']-30/60/24, timezone=tz_utc))\n",
    "\n",
    "ax.set_xlim(mjd_to_datetime(night_events['sunset']+30/60/24), \n",
    "         mjd_to_datetime(night_events['sunrise']-30/60/24))\n",
    "ax_utc.set_xlim(mjd_to_datetime(night_events['sunset']+30/60/24, timezone=tz_utc), \n",
    "         mjd_to_datetime(night_events['sunrise']-30/60/24, timezone=tz_utc))\n",
    "\n",
    "# Set ticks relevant sides\n",
    "ax.tick_params(axis=\"x\", bottom=True, top=False, labelbottom=True, labeltop=False)\n",
    "ax_utc.tick_params(axis=\"x\", bottom=False, top=True, labelbottom=False, labeltop=True)\n",
    "\n",
    "# Rotate and align bottom ticklabels\n",
    "plt.setp([tick.label1 for tick in ax.xaxis.get_major_ticks()], rotation=45,\n",
    "         ha=\"right\", va=\"center\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Rotate and align top ticklabels\n",
    "plt.setp([tick.label2 for tick in ax_utc.xaxis.get_major_ticks()], rotation=45,\n",
    "         ha=\"left\", va=\"center\",rotation_mode=\"anchor\")\n",
    "\n",
    "plt.grid(True, alpha=0.2)\n",
    "\n",
    "plt.ylim(2.3, 0.9)\n",
    "\n",
    "ax.set_ylabel(\"Airmass\", fontsize=\"large\")\n",
    "ax.set_xlabel(f\"Time ({tz})\", fontsize=\"large\")\n",
    "ax_utc.set_xlabel(\"Time (UTC)\", fontsize='large')\n",
    "_ = plt.ylabel(\"Airmass\", fontsize=\"large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bab81a-fea1-4173-acd7-003b24190b0a",
   "metadata": {},
   "source": [
    "Visits from the ConsDB, as well as `Targets` and `Observations` from the EFD records generated from the FeatureBasedScheduler. <br>\n",
    "The plot above stretches from sunset to sunrise, with civil, -12 degree and -18 degree sunset and sunrise indicated by the intensity of the gray shading. A blue (red) dashed line indicates the time of moonrise (moonset), if applicable. \n",
    "\n",
    "Blue shaded regions (if present) indicate periods of time with multiple sequential `Targets` that failed to find an expected `Observation` record in the EFD, and are numbered T-X. <br>\n",
    "Pink shaded regions (if present) indicate larger than expected overheads between sequential `Observation` events, and are numbered D-X. <br> \n",
    "Periods with both a delay of target and a series of sequential target failures are shaded both pink and blue, which results in lavender regions (but these are not double-counted above in possible issue time). <br>\n",
    "A orange shaded region (if present) on the left of the plot indicates the time from sunset until the time the first `Target` is sent from the FBS.\n",
    "\n",
    "`Targets` which have been linked with a corresponding `Observation` event (indicating the observing script completed successfully) are indicated by solid circles. `Targets` which were not able to be linked to an `Observation` are shown by crosses. \n",
    "\n",
    "`Visits` for `OBJECT` images are indicated by open circles, while `ACQ` `visits` are indicated by stars; `FOCUS` `visits` are shown with diamonds while `CWFS` `visits` are shown by hexagons.\n",
    "\n",
    "Both `Targets`, `Observations` and `Visits` are color-coded by their `note` values which are constructed in this notebook (using a combination of science_program and target_name). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2431ab-1541-455a-b883-53124def058d",
   "metadata": {},
   "source": [
    "<a id=\"Night_report\"></a>\n",
    "\n",
    "## Night Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55e3d1e-6d8f-4c30-8fab-5f1557ba48f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# night plan\n",
    "with open(\"/home/l/lynnej/.zephyr_token\", \"r\") as f:\n",
    "    zephyr_token = f.read().rstrip(\"\\n\")\n",
    "zephyr_url = \"https://api.zephyrscale.smartbear.com/v2/testcycles\"\n",
    "headers = {\"Accept\": \"application/json\",\n",
    "           \"Authorization\": f\"Bearer {zephyr_token}\",\n",
    "           \"Content-Type\": \"application/json\"}\n",
    "params = {\"maxResults\": 100,\n",
    "           \"startAt\": 100,\n",
    "          \"projectKey\": \"BLOCK\"} \n",
    "response = requests.get(url=zephyr_url, headers=headers, params=params)\n",
    "results = response.json()['values']\n",
    "test_cycle = pd.DataFrame(results).query('name.str.contains(@day_obs)')\n",
    "key = test_cycle['key'].values[0]\n",
    "url = \"https://rubinobs.atlassian.net/projects/BLOCK?selectedItem=com.atlassian.plugins.atlassian-connect-plugin:com.kanoah.test-manager__main-project-page#!/testPlayer/\" + key\n",
    "display(HTML(\"<strong>Test cycle / night plan</strong>\"))\n",
    "display(HTML(f'{day_obs}, <a href=\"{url}\" target=\"_blank\" rel=\"noreferrer noopener\">{key}</a>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f7933-e65f-4251-9d55-f014730ae501",
   "metadata": {},
   "outputs": [],
   "source": [
    "if telescope.startswith(\"Aux\"):\n",
    "    tel_nr = \"AuxTel\"\n",
    "else:\n",
    "    tel_nr = \"Simonyi\"\n",
    "\n",
    "this_dayobs = day_obs.replace('-', '')\n",
    "next_dayobs = (Time(day_obs, format='iso') + TimeDelta(1, format='jd')).iso[0:10].replace('-', '')\n",
    "\n",
    "params = {\"telescopes\" : tel_nr,\n",
    "          \"min_day_obs\" : this_dayobs,\n",
    "          \"max_day_obs\" : next_dayobs,\n",
    "          \"is_valid\" : \"true\",\n",
    "         }\n",
    "\n",
    "API_ENDPOINT = \"https://usdf-rsp-dev.slac.stanford.edu/nightreport/reports\"\n",
    "\n",
    "# we try twice because that's an issue with these services\n",
    "response = requests.get(API_ENDPOINT, params)\n",
    "if response.status_code != 200:\n",
    "    response = requests.get(API_ENDPOINT, params)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Night report service seems to be down\")\n",
    "\n",
    "else:    \n",
    "    logs = response.json()\n",
    "    \n",
    "    if len(logs) == 0:\n",
    "        print(\"No night report available.\")\n",
    "    \n",
    "    \n",
    "    if len(logs) > 0:\n",
    "        for log in logs:\n",
    "            display(Markdown(f\"Observing crew : {log['observers_crew']}\"))\n",
    "            night_plan_block = \"BLOCK\" + urllib.parse.urlparse(log['confluence_url']).fragment.split(\"BLOCK\")[-1]\n",
    "            if night_plan_block == \"BLOCK\":\n",
    "                night_plan_block = log['confluence_url']\n",
    "            display(Markdown(f\"Night plan : [{night_plan_block}]({log['confluence_url']})\"))\n",
    "            display(Markdown(\"<strong>Summary</strong>\"))\n",
    "            display(Markdown(log['summary']))\n",
    "            display(Markdown(\"<strong>Status</strong>\"))\n",
    "            display(Markdown(log['telescope_status']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9c82ae-1039-41dd-b3bd-9f48aa39184f",
   "metadata": {},
   "source": [
    "<a id=\"Narrative_exposure_logs\"></a>\n",
    "\n",
    "## Narrative Log Combined with EFD Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e638948-9841-4509-87c7-2de16c474886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the narrative log \n",
    "\n",
    "import requests\n",
    "\n",
    "if telescope.lower().startswith(\"aux\"):\n",
    "    exclude_components = [\"MTMount\", \"MainTel\"]\n",
    "\n",
    "else:\n",
    "    exclude_components = [\"AuxTel\", \"ATMCS\", \"ATDome\"]\n",
    "\n",
    "params = {\"is_human\" : \"either\",\n",
    "          \"is_valid\" : \"true\",\n",
    "          \"has_date_begin\" : True,\n",
    "          \"min_date_begin\" : early_setup.to_datetime(),\n",
    "          \"max_date_begin\" : sunrise.to_datetime(),\n",
    "          \"exclude_components\" : exclude_components,\n",
    "          #\"min_time_lost\" : 0.0001,\n",
    "          \"order_by\" : \"date_begin\",\n",
    "         }\n",
    "\n",
    "API_ENDPOINT = \"https://usdf-rsp-dev.slac.stanford.edu/narrativelog/messages\"\n",
    "\n",
    "# we try twice because that's an issue with these services\n",
    "response = requests.get(API_ENDPOINT, params)\n",
    "if response.status_code != 200:\n",
    "    response = requests.get(API_ENDPOINT, params)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Narrative log service seems to be down\")\n",
    "    messages = []\n",
    "\n",
    "else:\n",
    "    messages = response.json()\n",
    "    for m in messages:\n",
    "        m['message_text_mod'] = m['message_text'].replace(\"\\r\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\").rstrip(\"\\n\")\n",
    "    messages = pd.DataFrame(messages)\n",
    "    def make_time(x):\n",
    "        return Time(x['date_begin'], format='isot', scale='tai').to_datetime()\n",
    "    messages['time'] = messages.apply(make_time, axis=1)\n",
    "    messages.set_index('time', inplace=True)\n",
    "    messages.rename({'time_lost_type': 'error_type'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c2b71b-7a79-4e26-a5aa-b48da342cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get EFD Error messages\n",
    "topics = await efd_client.get_topics()\n",
    "if telescope.lower().startswith('aux'):\n",
    "    err_topics = [t for t in topics if 'err' in t and 'MT' not in t]\n",
    "else:\n",
    "    err_topics = [t for t in topics if 'err' in t and 'AT' not in t]\n",
    "\n",
    "\n",
    "errs = []\n",
    "for topic in err_topics:\n",
    "    df = await efd_client.select_time_series(topic, ['errorCode', 'errorReport'], early_setup, sunrise)\n",
    "    if len(df) > 0:\n",
    "        df['topic'] = topic\n",
    "        errs += [df]\n",
    "if len(errs) > 0:\n",
    "    errs = pd.concat(errs).sort_index()\n",
    "    errs['time'] = Time(errs.index)\n",
    "    errs.set_index('time', inplace=True)\n",
    "    errs.rename({'errorCode': 'error_type', 'errorReport': 'message_text_mod', 'topic': 'components'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4a2cd1-1bd9-4e4f-9d72-94af67a7a2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Civil sunset at {civil_sunset.isot}\")\n",
    "print(f\"12 degree sunset at {sunset.isot}\")\n",
    "print(f\"12 degree sunrise at {sunrise.isot}\")\n",
    "print()\n",
    "\n",
    "cols = ['components', 'message_text_mod', 'error_type']\n",
    "joint = []\n",
    "if len(errs) > 0 and len(messages) > 0:\n",
    "    print(\"Joint narrative log and error messages\")\n",
    "    joint = pd.concat([errs, messages]).sort_index()\n",
    "\n",
    "elif len(errs) > 0:\n",
    "    print(\"Error messages only; narrative log empty\")\n",
    "    joint = errs\n",
    "\n",
    "elif len(messages) > 0:\n",
    "    print(\"Narrative log only; error messages empty\")\n",
    "    joint = messages\n",
    "    \n",
    "if len(joint) > 0:\n",
    "    with option_context('display.max_colwidth', None):\n",
    "        display(HTML(joint[cols].to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf6b92c-998b-41cc-9ae1-b7038437a41f",
   "metadata": {},
   "source": [
    "## Exposure Log content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6837b538-5cc6-4ab7-8955-646d09b04ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the exposure log -- probably only want logs from exposures during and adjacent to breaks .. \n",
    "# but need to build that up later\n",
    "\n",
    "import requests\n",
    "\n",
    "if telescope.lower().startswith(\"aux\"):\n",
    "    exclude_components = [\"MTMount\", \"MainTel\"]\n",
    "\n",
    "else:\n",
    "    exclude_components = [\"AuxTel\", \"ATMCS\", \"ATDome\"]\n",
    "\n",
    "this_dayobs = day_obs_int\n",
    "next_dayobs = (Time(day_obs, format='iso') + TimeDelta(1, format='jd')).iso[0:10].replace('-', '')\n",
    "\n",
    "params = {\"is_human\" : \"either\",\n",
    "          \"is_valid\" : \"true\",\n",
    "          \"min_day_obs\" : this_dayobs,\n",
    "          \"max_day_obs\" : next_dayobs,\n",
    "          \"instrument\" : instrument.upper(),\n",
    "          \"order_by\" : \"seq_num\",\n",
    "         }\n",
    "\n",
    "API_ENDPOINT = \"https://usdf-rsp-dev.slac.stanford.edu/exposurelog/messages\"\n",
    "\n",
    "\n",
    "# we try twice because that's an issue with these services\n",
    "response = requests.get(API_ENDPOINT, params)\n",
    "if response.status_code != 200:\n",
    "    response = requests.get(API_ENDPOINT, params)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"exposure log seems to be down\")\n",
    "    logs = []\n",
    "    \n",
    "else:\n",
    "    logs = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be3985-9dd7-4105-8427-5199d928cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(logs) == 0:\n",
    "    display(Markdown(f\"No exposure log information available for {day_obs}\"))\n",
    "else:\n",
    "    cols = ['obs_id', 'instrument', 'day_obs', 'seq_num', 'user_id', 'user_agent', 'exposure_flag', 'message_text']\n",
    "    pd.DataFrame(logs)[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d74e47-5c6a-48a3-8dcd-d0073041dc2e",
   "metadata": {},
   "source": [
    "<!-- ## EFD Error messages\" -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e69e97-d3bd-400d-b3a0-2d17fdc8f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics = await efd_client.get_topics()\n",
    "# if telescope.lower().startswith('aux'):\n",
    "#     err_topics = [t for t in topics if 'err' in t and 'MT' not in t]\n",
    "# else:\n",
    "#     err_topics = [t for t in topics if 'err' in t and 'AT' not in t]\n",
    "\n",
    "\n",
    "# errs = []\n",
    "# for topic in err_topics:\n",
    "#     df = await efd_client.select_time_series(topic, ['errorCode', 'errorReport'], early_block, sunrise)\n",
    "#     if len(df) > 0:\n",
    "#         df['topic'] = topic\n",
    "#         errs += [df]\n",
    "# errs = pd.concat(errs).sort_index()\n",
    "# with option_context('display.max_colwidth', None):\n",
    "#     display(HTML(errs.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499506d9-d1b7-45d5-a105-db8a22d2ff00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

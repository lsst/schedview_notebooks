{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7929a2b2-4437-4185-ba12-20274eb9e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is only for setting example parameter defaults - gets replaced by sidecar.\n",
    "day_obs = \"2024-12-10\"\n",
    "day_obs = \"2024-08-13\"\n",
    "day_obs = \"Today\"\n",
    "telescope = \"AuxTel\"  \n",
    "#telescope = \"SimonyiTel\"\n",
    "instrument = \"latiss\"  \n",
    "#instrument = \"lsstcomcam\"\n",
    "salindex = 2\n",
    "#salindex = 1\n",
    "timezone = \"Chile/Continental\"\n",
    "#timezone = \"UTC\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac5b1f-b036-4213-ae6d-700dcad8a7a6",
   "metadata": {},
   "source": [
    "# Scheduler: Targets, Observations and ConsDB Visits for {{ params.day_obs }} {{ params.telescope }}\n",
    "\n",
    "The Feature Based Scheduler requests `Targets` and completed observation scripts result in `Observations`; these are both are tracked in the EFD.\n",
    "The Consdb reports acquired `Visits`.  \n",
    "On-sky exposures can also be acquired directly through execution of scripts or JSON BLOCKs; these don't result in `Targets` but do produce `Visits`.\n",
    "\n",
    "\n",
    "* [Almanac](#Almanac)\n",
    "* [EFD Configuration](#EFD_configuration)\n",
    "* [EFD Targets and Observations](#EFD_targets)\n",
    "* [ConsDB Visits](#Consdb_visits)\n",
    "* [Overheads and Issues](#Overheads)\n",
    "* [Summary Plot](#Summary_plot)\n",
    "* [Night Log Report](#Night_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17629b1c-626e-417f-8bdc-20332107534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import copy\n",
    "from math import floor\n",
    "import numpy as np\n",
    "import healpy as hp\n",
    "import matplotlib.pylab as plt\n",
    "from cycler import cycler\n",
    "import colorcet as cc\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import option_context\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "import datetime\n",
    "from astropy.time import Time, TimeDelta\n",
    "import astropy.units as u\n",
    "import astropy\n",
    "astropy.utils.iers.conf.iers_degraded_accuracy = 'ignore'\n",
    "\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "try:\n",
    "    tz = ZoneInfo(timezone)\n",
    "    tz_utc = ZoneInfo(\"UTC\")\n",
    "except ZoneInfoNotFoundError:\n",
    "    print(\"Timezone should be a string recognizable to `ZoneInfo`.\")\n",
    "    print(\"Using Chile/Continental (+UTC) backup.\")\n",
    "    tz = ZoneInfo(\"Chile/Continental\")\n",
    "    tz_utc = ZoneInfo(\"UTC\")\n",
    "\n",
    "from rubin_scheduler.site_models import Almanac\n",
    "from rubin_scheduler.utils import Site\n",
    "\n",
    "import requests\n",
    "import urllib\n",
    "import sqlalchemy\n",
    "from lsst_efd_client import EfdClient\n",
    "try:\n",
    "    from lsst.summit.utils import ConsDbClient\n",
    "    have_consdb = True\n",
    "except ImportError:\n",
    "    have_consdb = False\n",
    "\n",
    "# Access nightlog and JIRA\n",
    "try:\n",
    "    from lsst.rsp import get_access_token\n",
    "except ImportError:\n",
    "    def get_access_token(token_file : str | None = None) -> str:\n",
    "        token = os.environ.get(\"ACCESS_TOKEN\")\n",
    "        if token == None:\n",
    "            if token_file is not None:\n",
    "                with open(\"token_file\", \"r\") as f:\n",
    "                    token = f.read()\n",
    "            else:\n",
    "                warnings.warn(\"No RSP token available.\")\n",
    "        return token\n",
    "\n",
    "# for scheduler snapshots\n",
    "from urllib.parse import urlparse\n",
    "from lsst.resources import ResourcePath\n",
    "\n",
    "\n",
    "# at USDF or at summit?\n",
    "if os.getenv(\"EXTERNAL_INSTANCE_URL\", \"\") == \"https://summit-lsp.lsst.codes\":\n",
    "    efd = 'summit_efd'\n",
    "else:\n",
    "    efd = 'usdf_efd' \n",
    "    if 'usdf' in os.getenv(\"EXTERNAL_INSTANCE_URL\", \"\"):\n",
    "        os.environ[\"RUBIN_SIM_DATA_DIR\"] = \"/sdf/data/rubin/shared/rubin_sim_data\"\n",
    "        os.environ[\"no_proxy\"] += \",.consdb\"\n",
    "    \n",
    "# Not sure of summit consdb access, just use USDF for now\n",
    "os.environ[\"LSST_CONSDB_PQ_URL\"] = \"http://consdb-pq.consdb:8080/consdb\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eda54a-f5fa-4f1a-9068-af721e185471",
   "metadata": {},
   "outputs": [],
   "source": [
    "if day_obs.lower() == \"today\":\n",
    "    # Shift the 12hour offset following the definition of day_obs in https://sitcomtn-032.lsst.io/    \n",
    "    # Drop the hours, minutes, seconds to get the ISO formatted day_obs\n",
    "    day_obs = Time(np.floor(Time.now().mjd - 0.5), format='mjd', scale='utc').iso[0:10]\n",
    "\n",
    "elif day_obs.lower() == \"yesterday\":\n",
    "    # Shift the 12hour offset following the definition of day_obs in https://sitcomtn-032.lsst.io/\n",
    "    # Drop the hours, minutes, seconds to get the ISO fromatted day_obs\n",
    "    day_obs = (Time(np.floor(Time.now().mjd - 0.5), format='mjd', scale='utc') - TimeDelta(1, format='jd')).iso[0:10]\n",
    "\n",
    "else:\n",
    "    # test that day_obs is a proper day_obs\n",
    "    try:\n",
    "        test_day_obs = Time(f\"{day_obs}T12:00:00\", format='isot', scale='utc')\n",
    "    except ValueError:\n",
    "        msg = \"day_obs should be a date formatted as YYYY-MM-DD\"\n",
    "        raise ValueError(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e9430-0899-4218-896b-d6083667d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "minutes_to_days = 1./60/24\n",
    "seconds_to_days = 1./60/60/24"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0273835-d52b-4fb2-82d9-966e8612e57a",
   "metadata": {},
   "source": [
    "<a id=\"Almanac\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e75664-53f5-4748-83d4-479c1697db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Almanac ## \n",
    "\n",
    "display(Markdown(f\"## Almanac information for dayobs {day_obs}\"))\n",
    "site = Site('LSST')\n",
    "almanac = Almanac()\n",
    "night_events = almanac.get_sunset_info(evening_date=day_obs, longitude=site.longitude_rad)\n",
    "civil_sunset = Time(night_events['sunset'], format='mjd', scale='utc') \n",
    "sunset = Time(night_events['sun_n12_setting'], format='mjd', scale='utc') \n",
    "sunrise = Time(night_events['sun_n12_rising'], format='mjd', scale='utc')\n",
    "night_length = sunrise.mjd - sunset.mjd\n",
    "\n",
    "display(Markdown(f\"12-deg sunset at {sunset.to_datetime(timezone=tz_utc).strftime('%x %X')} UTC  -- {sunset.to_datetime(timezone=tz).strftime('%x %X')} {timezone}\"))\n",
    "display(Markdown(f\"12-deg sunrise at {sunrise.to_datetime(timezone=tz_utc).strftime('%x %X')} UTC --  {sunrise.to_datetime(timezone=tz).strftime('%x %X')} {timezone}\"))\n",
    "display(Markdown(f\"allowing for a night of {night_length * 24 :.2f} hours\"))\n",
    "moon_phase = almanac.get_sun_moon_positions(sunset.mjd)['moon_phase']\n",
    "if not np.isnan(night_events['moonrise']):\n",
    "    moonrise = Time(night_events['moonrise'], format='mjd', scale='utc')\n",
    "    display(Markdown(f\"Moonrise is at {moonrise.to_datetime(timezone=tz_utc).strftime('%x %X')} UTC -- {moonrise.to_datetime(timezone=tz).strftime('%x %X')} {timezone}\"))\n",
    "if not np.isnan(night_events['moonset']):\n",
    "    moonset = Time(night_events['moonset'], format='mjd', scale='utc')\n",
    "    display(Markdown(f\"Moonset at {moonset.to_datetime(timezone=tz_utc).strftime('%x %X')} UTC -- {moonset.to_datetime(timezone=tz).strftime('%x %X')} {timezone}\"))\n",
    "display(Markdown(f\"Moon phase is {moon_phase :.1f} (0=new, 100=full).\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe25532-a602-4ab4-95a1-575bef3dc455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report time notebook was run, which is likely useful if running notebook in the middle or start of the night\n",
    "current_time = Time.now()\n",
    "display(Markdown(f\"Time of notebook execution: {current_time.isot}\"))\n",
    "if current_time > sunrise:\n",
    "    display(Markdown(\"Night is complete.\"))\n",
    "elif current_time < sunrise and current_time > civil_sunset:\n",
    "    display(Markdown(\"Night is in progress.\"))\n",
    "elif current_time < civil_sunset:\n",
    "    display(Markdown(\"Night not yet started.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e60d9-302e-4f41-8f32-f5f9b4511c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "efd_client = EfdClient(efd)\n",
    "obsenv_client = EfdClient(efd, db_name='lsst.obsenv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8dd651-6b62-4eba-b6be-a5ce4ebea985",
   "metadata": {},
   "source": [
    "<a id=\"EFD_configuration\"></a>\n",
    "\n",
    "## EFD Configuration Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ff25f-8236-4285-82fc-af604ad82867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at civil sunset, in case of early use of scheduler\n",
    "early_setup = civil_sunset\n",
    "\n",
    "#display(Markdown(f\"## EFD Information\")) \n",
    "\n",
    "# What versions of the Scheduler modules are being used\n",
    "display(Markdown(\"### Scheduler Versions\"))\n",
    "\n",
    "# Scheduler dependency information\n",
    "async def get_scheduler_configs(t_start: Time, t_end: Time, efd_client: EfdClient, obsenv_client: EfdClient, queueIndex: int | None =None) -> pd.DataFrame:\n",
    "    \"\"\"Return information needed to recreate FBS configuration. \n",
    "\n",
    "    This requires checking the obsenv (`lsst.obsenv.summary`) to find the version of ts_config_ocs in use, \n",
    "    the EFD (`lsst.sal.Scheduler.logevent_dependenciesVersions`) to find the version of rubin_scheduler and dependencies,\n",
    "    and the EFD (`lsst.sal.Scheduler.logevent_configureApplied`) to find the specific FBS configuration file in use.\n",
    "\n",
    "    Searches both the time within t_start to t_end, as well as the last configuration applied before this time period. \n",
    "    \n",
    "    Defining queueIndex will search dependencies and configurations for that queue only. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t_start : `astropy.Time`\n",
    "        The time of the start of the period. \n",
    "    t_end : `astropy.Time`\n",
    "        The time at the end of the period.\n",
    "    efd_client : `lsst.efd.EfdClient`\n",
    "        An EFD client pointed to the standard EFD database.\n",
    "    obsenv_client : `lsst.efd.EfdClient`\n",
    "        An EFD client pointed to the obsenv database.\n",
    "    queueIndex : `int` or `None`\n",
    "        The salIndex of a specific queue (1=Simonyi, 2=Auxtel, 3=OCS).\n",
    "        If None, queries all queues, but the initial state may be missed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sched_config : `pd.DataFrame`\n",
    "        A dataframe carrying the configuration information.\n",
    "        Some columns are compacted into single strings, so \n",
    "        the entire dataframe can fit into a limited set of columns.\n",
    "    \"\"\"\n",
    "    # First find the obsenv to find the version of ts_config_ocs\n",
    "    topic = 'lsst.obsenv.summary'\n",
    "    fields = ['summit_extras', 'summit_utils',  'ts_standardscripts', 'ts_externalscripts', 'ts_config_ocs']\n",
    "    obsenv_start = await obsenv_client.select_top_n(topic, fields, num=1, time_cut=t_start.isot)\n",
    "    obsenv = await obsenv_client.select_time_series(topic, fields, t_start, t_end)\n",
    "    obsenv = pd.concat([obsenv_start, obsenv])\n",
    "    if len(obsenv) == 0:\n",
    "        warnings.warn(\"Could not find obsenv values.\")\n",
    "        # This shouldn't happen, but could before obsenv was implemented.\n",
    "        # We need something to fill in for work below.\n",
    "        bad_obsenv0 = [(t_start - TimeDelta(1, format='mjd') * 3).utc.datetime] + ['unknown' for f in fields]\n",
    "        bad_obsenv1 = [t_start.utc.datetime] + ['unknown' for f in fields]\n",
    "        obsenv = pd.DataFrame([bad_obsenv0, bad_obsenv1], columns=['time'] + fields)\n",
    "        obsenv.set_index('time', inplace=True)\n",
    "        obsenv.index = obsenv.index.tz_localize(\"UTC\")\n",
    "\n",
    "    # Label whether it was an obsenv *update* (i.e. changed ts_config_ocs, etc)\n",
    "    # Or just an obsenv *check* without update \n",
    "    # (obsenv entries are triggered by a command at the summit, which could be either of these jobs)\n",
    "    check = np.all((obsenv[fields][1:].values == obsenv[fields][:-1].values), axis=1)\n",
    "    classname = np.where(check, \"Obsenv Check\", \"Obsenv Update\")\n",
    "    obsenv['classname'] = np.concatenate([np.array(['Obsenv']), classname])\n",
    "    # Reconfigure some of the values to match the dataframe shape for logs \n",
    "    obsenv['description'] = (\"ts_config_ocs: \" + obsenv['ts_config_ocs'])\n",
    "    # Build compact config string\n",
    "    obsenv['config'] = (\"ts_standardscripts: \" + obsenv['ts_standardscripts'] + \n",
    "                        \"; ts_externalscripts: \" + obsenv['ts_externalscripts'] + \n",
    "                        \"; summit_utils: \" + obsenv['summit_utils'] + \n",
    "                        \"; summit_extras: \" + obsenv['summit_extras'])\n",
    "    # The obsenv is shared across all scriptqueues. The salIndex has to apply to all.\n",
    "    obsenv['salIndex'] = 0\n",
    "    obsenv['script_salIndex'] = -1\n",
    "\n",
    "    # Scheduler dependency information - updated independently of obsenv.\n",
    "    topic = 'lsst.sal.Scheduler.logevent_dependenciesVersions'\n",
    "    fields = ['cloudModel', 'downtimeModel', 'seeingModel', 'skybrightnessModel', 'observatoryLocation', 'observatoryModel', 'scheduler', 'salIndex', 'version']\n",
    "    deps_start = await efd_client.select_top_n(topic, fields, num=1, time_cut=t_start.isot, index=queueIndex)  \n",
    "    deps = await efd_client.select_time_series(topic, fields, t_start, t_end, index=queueIndex)    \n",
    "    deps = pd.concat([deps_start, deps])\n",
    "    if len(deps) == 0:\n",
    "        warnings.warn(\"Could not find scheduler dependencies.\")\n",
    "        bad_deps = [t_start.utc.datetime] + ['unknown' for f in fields]\n",
    "        deps = pd.DataFrame(bad_deps, columns=['time'] + fields)\n",
    "        deps.set_index('time', inplace=True)\n",
    "        deps.index = deps.index.tz_localize(\"UTC\")\n",
    "    \n",
    "    # Reconfigure output to fit into script_status fields \n",
    "    deps['classname'] = \"Scheduler dependencies\"\n",
    "    deps['description'] = f\"{deps['scheduler'].values[0]} {deps['seeingModel'].values[0]}\"\n",
    "    models = [c for c in deps.columns if 'observatory' in c or 'Model' in c]\n",
    "    def build_compact_config_string(x, models): \n",
    "        dep_string = ''\n",
    "        for m in models:\n",
    "            dep_string += f\"{m}: {x[m]}, \"\n",
    "        dep_string = dep_string[:-2]\n",
    "        return dep_string\n",
    "    deps['config'] = deps.apply(build_compact_config_string, args=[models], axis=1)\n",
    "    deps['script_salIndex'] = -1\n",
    "    \n",
    "    # The configurationApplied should happen with every scheduler update\n",
    "    topic = 'lsst.sal.Scheduler.logevent_configurationApplied'\n",
    "    fields = ['SchedulerId', 'configurations', 'salIndex', 'schemaVersion', 'url', 'version']\n",
    "    conf_start = await efd_client.select_top_n(topic, fields, num=1, time_cut=t_start.isot, index=queueIndex)\n",
    "    conf = await efd_client.select_time_series(topic, fields, t_start, t_end, index=queueIndex)\n",
    "    conf = pd.concat([conf_start, conf])\n",
    "    if len(conf) == 0:\n",
    "        warnings.warn(\"Could not find scheduler configuration.\")\n",
    "        bad_conf = [t_start.utc.datetime] + ['unknown' for f in fields]\n",
    "        conf = pd.DataFrame(bad_conf, columns=['time'] + fields)\n",
    "        conf.set_index('time', inplace=True)\n",
    "        conf.index = conf.index.tz_localize(\"UTC\")\n",
    "        \n",
    "    conf['classname'] = \"Scheduler configuration\"\n",
    "    # To get the scheduler relevant info in a single line, pull in ts_config_ocs\n",
    "    # to the configuration information.\n",
    "    ts_config_ocs_in_place = []\n",
    "    for time in conf.index:\n",
    "        prev_obsenv = obsenv.query('index < @time')\n",
    "        if len(prev_obsenv) == 0:\n",
    "            ts_config_ocs_in_place.append('Unknown')\n",
    "        else:\n",
    "            ts_config_ocs_in_place.append(prev_obsenv.iloc[-1]['ts_config_ocs'])\n",
    "    conf['ts_config_ocs'] = ts_config_ocs_in_place\n",
    "    def build_compact_description_string(x):\n",
    "        desc_string = x.configurations.split(',')[-1]  + \"<br> ts_config_ocs \" +  x.ts_config_ocs\n",
    "        link = f\"https://github.com/lsst-ts/ts_config_ocs/tree/{x.ts_config_ocs}/Scheduler/feature_scheduler\"\n",
    "        url = f'<a href=\"{link}\" target=\"_blank\" rel=\"noreferrer noopener\">{desc_string}</a>'\n",
    "        return url\n",
    "    conf['description'] = conf.apply(build_compact_description_string, axis=1)\n",
    "    conf.rename({'configurations': 'config'}, axis=1, inplace=True)\n",
    "    conf['script_salIndex'] = -1\n",
    "\n",
    "    # Combine results\n",
    "    sched_config = pd.concat([deps, conf, obsenv])\n",
    "\n",
    "    # Reformat\n",
    "    cols = ['classname', 'description', 'config', 'salIndex', 'script_salIndex']\n",
    "    drop_cols = [c for c in sched_config.columns if c not in cols]\n",
    "    sched_config.drop(drop_cols, axis=1, inplace=True)\n",
    "    sched_config.sort_index(inplace=True)\n",
    "    sched_config['timestampProcessStart'] = sched_config.index.copy().tz_localize(None).astype('datetime64[ns]')\n",
    "    sched_config['finalScriptState'] = \"Configuration\"\n",
    "    print(f\"Found {len(sched_config)} scheduler configuration records\")\n",
    "    return sched_config\n",
    "\n",
    "dd = await get_scheduler_configs(early_setup, sunrise, efd_client, obsenv_client, queueIndex=salindex)\n",
    "cols = dd.columns\n",
    "display(HTML(dd[cols].to_html(escape=False)))\n",
    "\n",
    "print(dd.iloc[-1]['description'])\n",
    "\n",
    "\n",
    "# What other BLOCKS have been requested in this night (outside the FBS)\n",
    "display(Markdown(\"### JSON BLOCKS\"))\n",
    "#topic = 'lsst.sal.Scheduler.command_addBlock'\n",
    "topic = 'lsst.sal.Scheduler.logevent_blockStatus'\n",
    "fields = await efd_client.get_fields(topic)\n",
    "fields = [f for f in fields if 'private' not in f]\n",
    "dd = await efd_client.select_time_series(topic, fields, early_setup, sunrise, index=salindex)\n",
    "if len(dd) == 0:\n",
    "    print(f\"No JSON BLOCKS added between {early_setup.iso} and {sunrise.iso}\")\n",
    "else:\n",
    "    grouped_dd = dd.groupby('id')[['id', 'definition', 'executionsCompleted', 'hash', 'salIndex']].agg('max')\n",
    "    grouped_dd_start = dd.groupby('id')[['id', 'definition', 'executionsCompleted', 'hash', 'salIndex']].agg('min')\n",
    "    grouped_dd['night_executions']  = grouped_dd['executionsCompleted'] - grouped_dd_start['executionsCompleted']\n",
    "    display(grouped_dd[['id', 'definition', 'hash', 'executionsCompleted', 'night_executions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c868a9b8-bedf-40ac-9ec8-e83c48495ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your JIRA Cloud base URL for API\n",
    "ZEPHYR_BASE_URL = \"https://api.zephyrscale.smartbear.com/v2/\" + \"testcases/\"\n",
    "JIRA_BASE_URL = \"https://rubinobs.atlassian.net/rest/api/2/\" + \"issue/\"\n",
    "\n",
    "testcase_base_url = \"https://rubinobs.atlassian.net/projects/BLOCK?selectedItem=com.atlassian.plugins.atlassian-connect-plugin:com.kanoah.test-manager__main-project-page#!/v2/testCase/\"\n",
    "\n",
    "jiraticket_base_url = \"https://rubinobs.atlassian.net/browse/\"\n",
    "\n",
    "if len(dd) > 0:\n",
    "    display(Markdown(\"For more information of test cases and blocks:\"))\n",
    "    for block in grouped_dd.id:\n",
    "        if block is not None and block.startswith(\"BLOCK-T\"):\n",
    "            url = testcase_base_url + block\n",
    "        elif block is not None and block.startswith(\"BLOCK-\"):\n",
    "            url = jiraticket_base_url + block\n",
    "        display(Markdown(f'<a href=\"{url}\" target=\"_blank\" rel=\"noreferrer noopener\">{block}</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87986df-81f5-42bf-b75f-c45b09a9c2d7",
   "metadata": {},
   "source": [
    "<a id=\"EFD_targets\"></a>\n",
    "\n",
    "## EFD Targets and Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfb48b5-6645-447c-a714-ead55c6ba536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch requested targets\n",
    "topic = 'lsst.sal.Scheduler.logevent_target'\n",
    "all_fields = await efd_client.get_fields(topic)\n",
    "#print(all_fields)\n",
    "targets = await efd_client.select_time_series(topic, all_fields, sunset, sunrise, index=salindex)\n",
    "\n",
    "def demangle_note_old(x):    \n",
    "    # remove _expnum\n",
    "    x.target = copy.deepcopy(x.note)\n",
    "    if \"IM\" in x.note:\n",
    "        x.note = x.note.split(\":\")[1].split(\"_\")[0]\n",
    "    if 'spec' in x.note:\n",
    "        x.note = 'HD' + x.note.split('HD')[-1]\n",
    "    return x\n",
    "\n",
    "# New note name -- still only note \n",
    "# But now note doubles for block + target name\n",
    "def demangle_note(x):    \n",
    "    vals = x.note.split(\":\")\n",
    "    block = x.note.split(\":\")[0]\n",
    "    if len(vals) > 1:\n",
    "        target_name = x.note.split(':')[1]\n",
    "        if \"HD\" in target_name: \n",
    "            # spec target\n",
    "            pass\n",
    "        elif \"_\" in target_name: \n",
    "            # most likely image target with tiles\n",
    "            # for compactness, remove tile\n",
    "            target_name = target_name.split(\"_\")[0]\n",
    "        x.science_program = block\n",
    "        x.target_name = target_name\n",
    "        x.note = target_name\n",
    "    else:\n",
    "        x.science_program = block\n",
    "        x.target_name = block\n",
    "        x.note = block\n",
    "    if x.note == \"BLOCK-305\":\n",
    "        x.note = 'cwfs'\n",
    "    return x\n",
    "\n",
    "if len(targets) == 0:\n",
    "    display(Markdown(f\"On night {day_obs} {telescope} recorded {len(targets)} target log events.\"))\n",
    "    display(Markdown(f\"This was for times between {sunset.iso}, {sunrise.iso}\"))\n",
    "\n",
    "    # debug help -- \n",
    "    targets = await efd_client.select_top_n(topic, all_fields, 2, index=salindex)\n",
    "    display(Markdown(\"The most recent targets were recorded were:\"))\n",
    "    display(targets[['SchedulerID', 'airmass', 'ra', 'decl', 'skyAngle', 'exposureTimes0', 'skyBrightness', 'slewTime', 'note', 'salIndex']])\n",
    "    targets = []\n",
    "\n",
    "else:\n",
    "\n",
    "    # target timestamp in EFD = time that the target is pushed to scriptqueue\n",
    "    # this should be at the start of the previous observation\n",
    "    targets[\"target_time\"] = targets.index.copy()\n",
    "    targets[\"target_mjd\"] = Time(targets.index).mjd\n",
    "    # estimate what time this target should be observed (start of observation)\n",
    "    # == target_time + previous exposure time (?) + slew time\n",
    "    cols = [c for c in targets if 'exposureTimes' in c]\n",
    "    targets['total_exptime'] = targets[cols].sum(axis=1)\n",
    "    \n",
    "    # Sometimes there are targets which don't correspond to a real target\n",
    "    # These seem to be triggered every time the Scheduler comes to ENABLED\n",
    "    targets = targets.query('total_exptime > 0')\n",
    "    \n",
    "    previous_exposure_time = np.concatenate([np.array([0]), targets['total_exptime'][:-1]])\n",
    "    # The target is supposed to be issued to the EFD when it hits the top of the scriptqueue \n",
    "    # which is supposed to be when the previous observation starts ... however\n",
    "    # an observation having many scripts may mean it doesn't hit the top of the scriptqueue then.\n",
    "    # (so this is probably more like a range of target.mjd + slewtime --- target + slewtime + previous exposure\n",
    "    targets[\"previous_exptime\"] = previous_exposure_time\n",
    "    targets[\"target_obsmjd\"] = Time(targets.index).mjd  + (targets['slewTime'])*seconds_to_days \n",
    "\n",
    "    # When estimating obsmjd expected from target -- include the previous\n",
    "    # exposure time or not? (in theory, should. in practice, queue is busy so not including it can be helpful).\n",
    "    include_prev_exptime = True\n",
    "\n",
    "    if include_prev_exptime:\n",
    "        targets[\"target_obsmjd\"] += (targets[\"previous_exptime\"])*seconds_to_days\n",
    "    \n",
    "    # demangle the note \n",
    "    targets['orig_note'] = targets.note.copy()\n",
    "    targets['target_name'] = ''\n",
    "    targets['block'] = ''\n",
    "    targets = targets.apply(demangle_note, axis=1)\n",
    "    \n",
    "    targets = targets.sort_values(by='target_obsmjd')\n",
    "    targets['target_id'] = np.arange(0, len(targets), 1)\n",
    "    targets = targets.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    targets.reset_index()\n",
    "    \n",
    "    display(Markdown(f\"On night {day_obs} {telescope} recorded {len(targets)} `target` events for \\n{targets.target_name.unique()}\"))\n",
    "\n",
    "\n",
    "# Check how many snapshots too as this equals number of calls\n",
    "topic = \"lsst.sal.Scheduler.logevent_largeFileObjectAvailable\"\n",
    "fields = [\"url\"]\n",
    "snapshots = await efd_client.select_time_series(topic, fields, sunset, sunrise, index=salindex)\n",
    "if len(snapshots) > 0:\n",
    "    display(Markdown(f\"There were {len(snapshots)} snapshots recorded\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899fb51-8014-4056-b3a7-e25bef49f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(HTML(targets[['target_id', 'target_obsmjd', 'ra', 'decl', 'slewTime', 'filter', 'exposureTimes0', 'note', 'orig_note']].to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905a1ca1-6cae-46f6-8252-e2b143a58167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(HTML(pd.concat([snapshots, targets.set_index('target_time')[['target_mjd', 'ra', 'decl', 'filter', 'skyAngle']]]).sort_index().to_html()))\n",
    "#display(HTML(targets[['target_mjd', 'ra', 'decl', 'filter', 'skyAngle', 'slewTime']].to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866dc7ca-4e42-4a70-bb92-b5c875634fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations are the completed observation script .. to be compared with visits from the consdb\n",
    "\n",
    "topic = 'lsst.sal.Scheduler.logevent_observation'\n",
    "all_fields = await efd_client.get_fields(topic)\n",
    "obs = await efd_client.select_time_series(topic, all_fields, sunset, sunrise, index=salindex)\n",
    "\n",
    "if len(obs) == 0:\n",
    "    display(Markdown(f\"On night {day_obs} {telescope} recorded {len(obs)} observation log events.\"))\n",
    "\n",
    "else:\n",
    "    # timestamp is the time of successful end of observation/JSON block\n",
    "    obs['obs_time'] = obs.index.copy()\n",
    "    obs['obs_mjd'] = Time(obs.index).mjd \n",
    "\n",
    "    # obs time - exposure time should be start of observation (if exposure time is correct)\n",
    "    # Looking at values recorded, exptime * nexps is not total exposure time (exptime alone is)\n",
    "    obs['obs_obsmjd'] = Time(obs.index).mjd - (obs['exptime'] * seconds_to_days)\n",
    "    \n",
    "    obs = obs.sort_values(by='obs_obsmjd')\n",
    "    obs['obs_id'] = np.arange(0, len(obs), 1)\n",
    "    obs = obs.reset_index(drop=True)\n",
    "    \n",
    "    display(Markdown(f\"On night {day_obs} {telescope} recorded {len(obs)} `observation` events.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5ca88-b1c9-4900-8367-23281a1acbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(HTML(obs[['ra', 'decl', 'exptime', 'filter', 'mjd', 'nexp', 'salIndex', 'targetId', 'obs_mjd',]].to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4e886-e93c-47d9-b724-d96a77ac52f2",
   "metadata": {},
   "source": [
    "### Matching `targets` to `observations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea280c-c029-4689-8f2f-70c26f503a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_obs_and_targets(obs, targets):\n",
    "    # This should be carried in the efd information - observation has a field for targetId\n",
    "    if len(targets) == 0:\n",
    "        print(\"No targets\")\n",
    "        return obs, targets\n",
    "    if len(obs) == 0:\n",
    "        print(\"No observations\")\n",
    "        targets['obs_id'] = -1\n",
    "        return obs, targets\n",
    "        \n",
    "    # Check targets -> observations\n",
    "    target_to_obs_match = np.zeros(len(targets), int) - 1\n",
    "    obs_to_target_match = np.zeros(len(obs), int) - 1\n",
    "    \n",
    "    print(\"Matching observations against targets\")\n",
    "    count = 0\n",
    "\n",
    "    # I'm finding that the first target can be very much delayed\n",
    "    # so let's treat that one separately\n",
    "    \n",
    "    for i, (ri, t) in enumerate(targets.iterrows()):\n",
    "        # Match obs_mjd, ra/dec/filter from target+observation\n",
    "        # the target.obs_mjd may overestimate actual mjd by ~ previous exposure\n",
    "        # if slewtime is inaccurate, target_obs.mjd may be inaccurate\n",
    "        # if visit includes JSON BLOCK with many steps, could be delays \n",
    "        # so that obs_obsmjd is later than reality\n",
    "        # some targets may never get completed observations\n",
    "        slew_error = 3.0 * minutes_to_days\n",
    "        target_error = t.previous_exptime * seconds_to_days\n",
    "        random_error = 14 * minutes_to_days\n",
    "        # don't match to a repeat of the same target much later\n",
    "        X = 30 * minutes_to_days\n",
    "        # CWFS target matching should try to use nearest time rather than ra/dec ? \n",
    "        if t.note == 'BLOCK-305':\n",
    "            delta_t = np.abs(t.target_obsmjd - obs.obs_obsmjd)\n",
    "            match = np.where((delta_t - delta_t.min() < 1e-2) # closest in time\n",
    "                             & ((obs.obs_obsmjd - t.target_obsmjd) <= X) # not more than X\n",
    "                             & (t['filter'] == obs['filter'])\n",
    "                             & (t['total_exptime'] == obs['exptime'])\n",
    "                             & (obs_to_target_match == -1))[0]\n",
    "        else:\n",
    "            match = np.where((np.abs(t.target_obsmjd - obs.obs_obsmjd) < target_error + slew_error + random_error) # close in time\n",
    "                             & ((obs.obs_obsmjd - t.target_obsmjd) <= X) # not more than X\n",
    "                             & (t.ra == obs.ra) \n",
    "                             & (t.decl == obs.decl) \n",
    "                             & (t['filter'] == obs['filter'])\n",
    "                             & (obs_to_target_match == -1))[0]\n",
    "\n",
    "        if len(match) == 0:\n",
    "            print(f'no obs match for target {t.target_id} {t.note}')\n",
    "            count += 1\n",
    "            target_to_obs_match[i] = -1\n",
    "        else:\n",
    "            # Consider target to have found the best match \n",
    "            # in the first (soonest) of 'match'\n",
    "            idx = match[0]\n",
    "            target_to_obs_match[i] = obs.iloc[idx]['obs_id']\n",
    "            # And consider observation to have found a match\n",
    "            obs_to_target_match[idx] = t['target_id']\n",
    "\n",
    "    obs['target_id'] = np.array(obs_to_target_match)\n",
    "    targets['obs_id'] = np.array(target_to_obs_match)\n",
    "    \n",
    "    print(f'failed to match {count} targets to observations')\n",
    "    print(f'compare to {len(targets) - len(obs)} expected difference')\n",
    "    \n",
    "    return obs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b43a-b9e0-41ef-aba2-892c6cfb33f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short columns helpful for cross-matching tests\n",
    "target_cols = ['ra', 'decl', 'skyAngle', 'filter', 'total_exptime', 'note', 'target_obsmjd']\n",
    "obs_cols  = ['ra', 'decl', 'rotSkyPos', 'filter', 'exptime', 'nexp', 'obs_obsmjd',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8df77e-41bc-4eca-beb3-d3b6acab871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, targets = match_obs_and_targets(obs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0052ce25-8d0b-4762-9f15-5fde1cd2882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets which did not get observations\n",
    "if len(targets) > 0:\n",
    "    tt = targets.query('obs_id == -1')[['ra', 'decl', 'skyAngle', 'filter', 'total_exptime', 'note', 'orig_note', 'target_mjd', 'obs_id',]]\n",
    "    if len(tt) > 0:\n",
    "        display(Markdown(\"The following `targets` were not able to be linked with `observations`\"))\n",
    "        display(tt)\n",
    "    else:\n",
    "        display(Markdown(\"All logged `targets` were linked with `observations`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf951f2-f79a-44e1-8724-b214687270e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(obs) > 0:\n",
    "    oo = obs.query('target_id == -1')[['ra', 'decl', 'rotSkyPos', 'filter', 'exptime', 'obs_mjd', 'target_id']]\n",
    "    if len(oo) > 0:\n",
    "        display(Markdown(\"The following `observations` were unable to be matched with `targets` (this is unusual).\"))\n",
    "        display(oo)\n",
    "    else:\n",
    "        display(Markdown(\"All `observations` were linked with `targets`\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1665de7-1903-4c50-867d-540b03b6a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join target and observations into one dataframe\n",
    "if len(obs) > 0:\n",
    "    # Join targets + obs\n",
    "    jj = obs.query('target_id > -1').join(targets, on='target_id', lsuffix='_o', rsuffix='_t')\n",
    "    jj = jj.sort_values(by='obs_obsmjd')\n",
    "    \n",
    "    jj['delta_obs'] = np.concatenate([np.array([0]), np.diff(jj['obs_obsmjd'])/minutes_to_days])  # minutes -- should be close to expected overhead\n",
    "    jj['delta_request'] = (jj['obs_obsmjd'] - jj['target_obsmjd']) / minutes_to_days  # minutes  -- should be close to 0\n",
    "    jj['expected_gap'] = (jj['previous_exptime']+ jj['slewTime'])/60  # minutes - expected overhead from models\n",
    "    anomalous_overhead = jj['delta_obs'].values - jj['expected_gap'].values\n",
    "    anomalous_overhead[0] = 0\n",
    "    jj['anomalous_overhead'] = anomalous_overhead\n",
    "else:\n",
    "    jj = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99de7cd-c46e-4ddb-9cf5-344ac9d68546",
   "metadata": {},
   "source": [
    "<a id=\"ConsDb_visits\"></a>\n",
    "\n",
    "## ConsDb Visit Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610d121-b75a-456c-b642-23aa29dd0d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add consdb\n",
    "\n",
    "day_obs_int = int(day_obs.replace('-', ''))\n",
    "\n",
    "visit_query = f'''\n",
    "    SELECT * FROM cdb_{instrument}.visit1\n",
    "     where day_obs = {day_obs_int}\n",
    "'''\n",
    "\n",
    "quicklook_query = f'''\n",
    "    SELECT q.*  FROM cdb_{instrument}.visit1_quicklook as q,\n",
    "    cdb_{instrument}.visit1 as v\n",
    "     WHERE v.day_obs = {day_obs_int} and q.visit_id = v.visit_id\n",
    "'''\n",
    "\n",
    "# Use the ConsDB Client, and add a couple of tries \n",
    "consdb = ConsDbClient()\n",
    "\n",
    "# Ugh, wrap the whole thing again in case the consdb is just down\n",
    "try: \n",
    "    try:\n",
    "        visits = consdb.query(visit_query).to_pandas()\n",
    "    except requests.HTTPError or requests.JSONDecodeError:\n",
    "        # Try twice\n",
    "        visits = consdb.query(visit_query).to_pandas()\n",
    "    \n",
    "    quicklook = consdb.query(quicklook_query).to_pandas()\n",
    "except requests.HTTPError:\n",
    "    display(Markdown(\"ConsDB seems to be unreachable\"))\n",
    "    visits = []\n",
    "    quicklook = []\n",
    "\n",
    "if len(visits) > 0:\n",
    "    display(Markdown(f\"Retrieved {len(visits)} visits from consdb\"))\n",
    "    obj_visits = visits.query('img_type == \"OBJECT\"')\n",
    "    display(Markdown(f\"{len(obj_visits)} of these are `OBJECT` images\"))\n",
    "\n",
    "if len(quicklook) > 0:\n",
    "    visits = visits.join(quicklook, on='visit_id', lsuffix='', rsuffix='_q')\n",
    "    visits = visits.copy()\n",
    "    display(Markdown(f\"And added quicklook stats\"))\n",
    "\n",
    "if len(visits) == 0:\n",
    "    display(Markdown(f\"No visits for {telescope} on {day_obs} retrieved from consdb\"))\n",
    "\n",
    "# Patch science_program if was None\n",
    "values = (dict([[e,\"\"] for e in ['science_program','target_name', 'observation_reason']]))\n",
    "visits.fillna(value=values, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0f668a-ec1b-4fc1-9b5a-4ca76845c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False\n",
    "short_cols = ['seq_num', 'obs_start_mjd', 'obs_end_mjd', 'exp_time', 'shut_time', 'dark_time', 's_ra', 's_dec', 'band', 'airmass', \n",
    "              'img_type', 'target_name', 'science_program', 'observation_reason', 'dimm_seeing']\n",
    "if len(visits)>0 and verbose:\n",
    "    display(visits[short_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286ec95-7e77-4deb-a7d3-036c3324da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a 'note' to match visits with the target/observation colors\n",
    "def construct_note_old(x):\n",
    "    if x.science_program == 'cwfs' or x.science_program == 'cwfs-focus-sweep':\n",
    "        note = 'cwfs'\n",
    "    elif x.science_program == \"BLOCK-305\":\n",
    "        note = 'cwfs'\n",
    "    elif x.science_program == \"BLOCK-295\":\n",
    "        note = 'calibrations'\n",
    "    elif x.target_name == 'FlatField position':\n",
    "        note = 'calibrations'\n",
    "    else:\n",
    "        note = x.target_name\n",
    "    if len(note) == 0:\n",
    "        note = 'unknown'\n",
    "    elif x.science_program == 'BLOCK-306':\n",
    "        note = x.target_name\n",
    "    elif x.science_program == 'spec-survey':\n",
    "        note = x.target_name\n",
    "    else:\n",
    "        note = 'unknown'\n",
    "    return note\n",
    "\n",
    "def construct_note(x):\n",
    "    if x.science_program == 'cwfs' or x.science_program == 'cwfs-focus-sweep' or x.science_program == \"BLOCK-305\":\n",
    "        note = 'cwfs'\n",
    "    elif x.img_type == 'ACQ':\n",
    "        note = 'acq'\n",
    "    elif x.science_program == \"BLOCK-295\":\n",
    "        note = 'calibrations'\n",
    "    elif x.target_name == 'FlatField position':\n",
    "        note = 'calibrations'\n",
    "    else:\n",
    "        if len(x.target_name) > 0: \n",
    "            note = x.target_name\n",
    "        elif len(x.science_program) > 0: \n",
    "            note = x.science_program\n",
    "        else:\n",
    "            note = x.observation_reason\n",
    "        if note is None:\n",
    "            note = 'unknown'\n",
    "    return note\n",
    "\n",
    "if len(visits)>0:\n",
    "    note = visits.apply(construct_note, axis=1)\n",
    "    visits['note'] = note\n",
    "    display(Markdown(f\"ConsDB constructed note names: {visits.note.unique()}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260540c0-4902-47b0-8b8b-73bba6b9b2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link consdb visits with targets .. note that this can be many visits -> one target\n",
    "# Also, should match against targets and not observations, because there can be some visits from an incomplete target\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    # probably shouldn't entirely suppress this, but can't quite find PerformanceWarning source\n",
    "    warnings.simplefilter('ignore')\n",
    "    if len(visits) > 0:\n",
    "        visit_target_id = np.zeros(len(visits)) - 1\n",
    "        if len(targets) > 0:\n",
    "            for note in targets.note.unique():\n",
    "                vv = visits.query('note == @note and science_program != None')\n",
    "                if len(vv) == 0:\n",
    "                    continue\n",
    "                tt = targets.query('note == @note')\n",
    "                # Find sequential visits \n",
    "                gaps = [[s, e] for s, e, in zip(vv.index, vv.index[1:]) if s+1 < e]\n",
    "                left = [vv.index[0]] + [g[1] for g in gaps]\n",
    "                right = [g[0] for g in gaps] + [vv.index[-1]]\n",
    "                consecutive = list(zip(left, right))\n",
    "                for l, r in zip(left, right):\n",
    "                    # if there are repeated targets with the same note, resulting in\n",
    "                    # sequential visits to the same note ... this will match all \n",
    "                    # the visits to the first target in the series (unfortunately)\n",
    "                    delta_t = vv.loc[l]['obs_start_mjd'] - tt['target_mjd']\n",
    "                    idx = np.where(delta_t > 0, delta_t, np.inf).argmin()\n",
    "                    visit_target_id[l:(r+1)] = tt.iloc[idx]['target_id']\n",
    "            \n",
    "        visits['target_id'] = visit_target_id\n",
    "\n",
    "    \n",
    "        display(Markdown(f\"Matched {len(visits.query('target_id > 1'))} `visits` to `targets`, out of a total of {len(visits)} visits.\"))\n",
    "        remainder = visits.query('target_id == -1')\n",
    "        display(Markdown(f\"The remaining {len(remainder)} visits were of type {remainder.img_type.unique()}\"))\n",
    "                         #f\" with science_programs {remainder.science_program.unique()}, observation_reason {remainder.observation_reason.unique()}.\"))\n",
    "\n",
    "# Matched!  .. could now compare time of actual visit with time of target and time of observation, try to sort out if there are timing issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de68f233-b582-48a6-ac81-1601562cb24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = None\n",
    "if len(visits) > 0:\n",
    "    groupcols = ['science_program', 'img_type', 'target_name', 'observation_reason', 'day_obs', 'visit_id'] \n",
    "    c = visits[groupcols].groupby(['science_program', 'img_type']).agg({'science_program' : ['first'],\n",
    "                                                                        'target_name' : ['unique'], \n",
    "                                                                        'observation_reason' : ['unique'],\n",
    "                                                                        'day_obs' : ['nunique'],\n",
    "                                                                        'visit_id' : ['first', 'last', 'count']})\n",
    "    display(Markdown(f\"ConsDB Visits\"))\n",
    "    with option_context('display.max_colwidth', None):\n",
    "        display(HTML(c.sort_values(by=('visit_id', 'first')).to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045c824-0359-454e-ac9d-4bed914cc192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a neat trick to dynamically create a download link .. but probably not necessary here \n",
    "\n",
    "# import base64\n",
    "\n",
    "# def create_download_link( df, title = \"Download CSV file\", filename = \"data.csv\"):\n",
    "#     csv = df.to_csv()\n",
    "#     b64 = base64.b64encode(csv.encode())\n",
    "#     payload = b64.decode()\n",
    "#     html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n",
    "#     html = html.format(payload=payload,title=title,filename=filename)\n",
    "#     return HTML(html)\n",
    "\n",
    "# display(create_download_link(targets, title=f\"Download linked targets CSV ({day_obs})\"))\n",
    "# display(create_download_link(obs, title=f\"Download linked observations CSV ({day_obs})\"))\n",
    "# display(create_download_link(visits, title=f\"Download linked visits CSV ({day_obs})\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b422b44-bdef-4e2f-81ea-569365a07c24",
   "metadata": {},
   "source": [
    "<a id=\"Overheads\"></a>\n",
    "\n",
    "## Overheads and Issues\n",
    "\n",
    "Comparing the expected overhead from the predicted slewtime and exposure time to the actual time recorded between observation events (where matched to targets). \n",
    "This can be used to identify longer-than-expected overheads between visits. \n",
    "Currently, this doesn't use the Visit information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2e407e-08c6-4264-838b-18fa19901b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "overhead_error = 3\n",
    "typical_anomalous_overhead = 0\n",
    "\n",
    "if len(jj) > 0:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    _ = ax[0].hist(jj['delta_obs'], bins=np.arange(-2, 25, 0.5), alpha=0.6, histtype='bar', label='delta observation obsmjd')\n",
    "    _ = ax[0].hist(jj['expected_gap'], bins=np.arange(-2, 25, 0.5), alpha=0.6, histtype='bar', label='predicted gaps between obs')\n",
    "    ax[0].legend(loc=(0.45, 0.85))\n",
    "    ax[0].set_xlabel(\"Minutes\", fontsize='large')\n",
    "\n",
    "    _ = ax[1].hist(jj['anomalous_overhead'], bins=50, histtype='bar', label=\"(delta_obs) - (pred_gap)\")\n",
    "    ax[1].axvline(overhead_error, color='r', linestyle=':')\n",
    "    ax[1].legend(loc=(0.58, 0.83))\n",
    "    ax[1].set_xlabel(\"Anomalous Overhead (minutes)\", fontsize='large')\n",
    "\n",
    "    typical_anomalous_overhead = np.median(jj.query('anomalous_overhead < @overhead_error')['anomalous_overhead'])\n",
    "    display(Markdown(f\"Median value of anomalous overhead, not including values beyond dashed line:  {typical_anomalous_overhead.round(2)} minutes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c3fd5-2621-400a-a148-938bf6fa2985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where did the time between observations just take longer than expected \n",
    "# delta obs compared to (slewTime + exptime) BUT sheduler target estimate was correct\n",
    "\n",
    "breaks = []\n",
    "if len(jj)>0: \n",
    "    overheads = np.where(jj.anomalous_overhead > overhead_error)[0]\n",
    "    \n",
    "    for b in overheads:\n",
    "        b_start = jj.iloc[b-1]['obs_obsmjd'] + jj.iloc[b-1]['exptime']/60/60/24\n",
    "        b_end = jj.iloc[b]['obs_obsmjd'] #- jj.iloc[b]['slewTime']/60/60/24\n",
    "        # look for cwfs sweeps around the break, as they seem to mess with timings\n",
    "        b_min = np.max([b-1, 0])\n",
    "        b_max = np.min([b+1, len(jj)-1])\n",
    "        check_for_cwfs = jj.iloc[b_min:b_max+1].query(\"note == 'cwfs'\")\n",
    "        if len(check_for_cwfs) > 0 and (b_end - b_start) < 10 * minutes_to_days:\n",
    "            # cwfs sweeps have variable timing and can disturb the expected timings\n",
    "            # but also, if we did have a long break, then count these anyway.\n",
    "            pass\n",
    "        else:\n",
    "            breaks.append([b_start, b_end])\n",
    "    \n",
    "    print(f\"There are {len(overheads)} points where the time between observations doesn't match predicted overheads within {overhead_error} minutes.\",\n",
    "           f\" CWFS visits are indicated in {len(overheads) - len(breaks)} of these breaks, and have a variable effect on the overhead so will be discounted.\")\n",
    "    print()\n",
    "    \n",
    "    display(Markdown(\"Details of unexpected deltas in targets-observations (prev - unexpected - next)\"))\n",
    "    for o in overheads:\n",
    "        display(jj[['target_id', 'obs_id_t', 'target_name', 'delta_obs', 'expected_gap', 'obs_obsmjd', 'target_obsmjd']][o-1:o+2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e70331-15e8-4583-8926-d5b2fb9c5aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(jj) > 0:\n",
    "    plt.figure()\n",
    "    \n",
    "    eps = 1\n",
    "    for note in jj.note.unique():\n",
    "        j = jj.query('note == @note')\n",
    "        plt.plot(j.delta_obs, j.expected_gap, '.', label=note)\n",
    "    j = jj.iloc[overheads]\n",
    "    plt.plot(j.delta_obs, j.expected_gap, 'o',\n",
    "             markersize=10, color='k', markerfacecolor='none', label='Big Overhead')\n",
    "    plt.legend(loc=(1.01, 0.5))\n",
    "    for exptime in jj.exptime.unique():\n",
    "        plt.axhline(exptime/60, color='gray', linestyle=':', alpha=0.6)\n",
    "    \n",
    "    x = np.arange(0, 60)\n",
    "    plt.plot(x, x, 'r:')\n",
    "    plt.fill_between(x, x-overhead_error, x+overhead_error,  color='r', alpha=0.1)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    max_x = np.max(jj.delta_obs) + eps\n",
    "    max_y = np.max(jj.expected_gap) + eps/2\n",
    "    if max_x > max_y + 10:\n",
    "        max_x = max_y + overhead_error\n",
    "    plt.xlim(0, max_x)\n",
    "    plt.ylim(0, max_y)\n",
    "    \n",
    "    plt.xlabel(\"DeltaT between start of observations (minutes)\")\n",
    "    plt.ylabel(\"Expected gap (exptime + slew) (minutes)\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57ed291-4f69-4f37-8e81-082ba9f8d6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify any sequences of incomplete targets, to color-code later\n",
    "breaks_targets = []\n",
    "if len(targets) > 0:\n",
    "    missing_break = 0\n",
    "    t = targets.query('obs_id == -1').index.values\n",
    "    if len(t) > 0:        \n",
    "        gaps = [[s, e] for s, e, in zip(t, t[1:]) if s+1 < e]\n",
    "        left = [t[0]] + [g[1] for g in gaps]\n",
    "        right = [g[0] for g in gaps] + [t[-1]]\n",
    "        consecutive = list(zip(left, right))\n",
    "        # Trim off the end, as a likely single-missed target\n",
    "        if consecutive[-1][0] == consecutive[-1][1]:\n",
    "            consecutive = consecutive[:-1]\n",
    "        for c_visits in consecutive:\n",
    "            start_idx = c_visits[0]\n",
    "            end_idx = c_visits[1] + 1\n",
    "            if end_idx > start_idx + 1: \n",
    "                # Use time target recorded at as the start\n",
    "                b_start = targets.iloc[start_idx]['target_mjd']\n",
    "                # Use time target would have expected observation as end point\n",
    "                if end_idx > len(targets) - 1:\n",
    "                    end_idx = end_idx - 1\n",
    "                b_end = targets.iloc[end_idx]['target_obsmjd']\n",
    "                breaks_targets.append([b_start, b_end])\n",
    "                missing_break += 1\n",
    "\n",
    "    if missing_break == 1:\n",
    "        display(Markdown(f\"There was {missing_break} period of consecutive incomplete `targets`. Could be due to weather or script failures.\"))\n",
    "    if missing_break > 1:\n",
    "        display(Markdown(f\"There were {missing_break} periods of consecutive incomplete `targets`. Could be due to weather or script failures.\"))\n",
    "    if missing_break == 0:\n",
    "        display(Markdown(\"There were no periods of consecutive incomplete `targets`.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ede609e-5c40-4ced-ae14-041fa6fe8dad",
   "metadata": {},
   "source": [
    "## Time Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e87400-c168-4498-afc6-e8b50d560a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try summing up time into different buckets, although it should be noted that this is still in need of more validation\n",
    "\n",
    "# First find when the ScriptQueue was running and enabled .. this could be slightly different than when Targets are sent.\n",
    "topic = 'lsst.sal.ScriptQueue.logevent_queue'\n",
    "fields = ['running', 'enabled', 'currentSalIndex', 'salIndices0', 'pastSalIndices0']\n",
    "dd = await efd_client.select_time_series(topic, fields, civil_sunset, sunrise)\n",
    "\n",
    "if len(dd) > 0:\n",
    "    # Parse out simonyi / auxtel / ocs entries from the scriptqueue\n",
    "    query = f\"pastSalIndices0.astype('str').str.startswith('{salindex}')\"\n",
    "    dd = dd.query(query)\n",
    "\n",
    "if len(dd) > 0:\n",
    "    dd['go'] = (dd['running'] & dd['enabled']).values\n",
    "    dd['go_prev'] = np.concatenate([np.array([False]), dd['go'].values[:-1]])\n",
    "    start = dd.query('(go != go_prev) and (go)').index.values\n",
    "    end = dd.query('(go != go_prev) and not (go)').index.values\n",
    "    if len(end) == len(start) - 1:\n",
    "        end = np.concatenate([end, dd.index[-1:].values])\n",
    "    start_queue = Time(start).mjd\n",
    "    end_queue = Time(end).mjd\n",
    "    time_queue_active = (end_queue - start_queue).sum()\n",
    "    sunset_to_queue = (start_queue[0] - sunset.mjd)\n",
    "else:\n",
    "    time_queue_active = 0\n",
    "    sunset_to_queue = night_length\n",
    "\n",
    "\n",
    "# Need to find time that the FBS is enabled and running\n",
    "\n",
    "\n",
    "# Let's assume we found the breaks accurately ... but also try not to double count\n",
    "tt = np.arange(sunset.mjd, sunrise.mjd, 1/60/24)\n",
    "val = np.zeros(len(tt))\n",
    "for b in breaks:\n",
    "    val = np.where((tt >= b[0]) & (tt <= b[1]), 1, val)\n",
    "for b in breaks_targets:\n",
    "    val = np.where((tt >= b[0]) & (tt <= b[1]), 1, val)\n",
    "time_in_breaks = val.sum() * minutes_to_days\n",
    "\n",
    "# Also look at when targets were being sent for this telescope\n",
    "if len(targets) > 0:\n",
    "    last_target = targets.iloc[-1]['target_mjd']\n",
    "    first_target = targets.iloc[0]['target_mjd']\n",
    "    time_with_targets = (last_target - first_target) - time_in_breaks\n",
    "    sunset_to_targets = (first_target - sunset.mjd)\n",
    "else:\n",
    "    time_with_targets = 0\n",
    "    sunset_to_targets = night_length\n",
    "\n",
    "# Predicted overhead in slews + anomalous overhead\n",
    "if len(jj) > 0:\n",
    "    time_overhead = (jj['slewTime'].sum() + jj['anomalous_overhead'].sum()) * seconds_to_days\n",
    "else:\n",
    "    time_overhead = 0\n",
    "\n",
    "\n",
    "# Count up observing time per science_program during the night\n",
    "# Just roll up OBJECT, CWFS, ACQ and FOCUS .. \n",
    "night_visits = 0\n",
    "if len(visits) > 0:\n",
    "    visits_after_sunset = visits.query(\"obs_start_mjd > @sunset.mjd\")\n",
    "    night_visits = len(visits_after_sunset)\n",
    "    if night_visits > 0:\n",
    "        programs = visits_after_sunset.science_program.unique()\n",
    "        onsky_time = {}\n",
    "        for p in programs:\n",
    "            onsky_time[p] = visits_after_sunset.query('science_program == @p').exp_time.sum() * seconds_to_days\n",
    "        # Consolidate \"random\" programs .. while getting metadata sorted\n",
    "        if '' in onsky_time:\n",
    "            onsky_time['Unknown'] = onsky_time['']\n",
    "            del onsky_time['']\n",
    "        sunset_to_visits = (visits_after_sunset.iloc[0].obs_start_mjd - sunset.mjd) \n",
    "        civilsunset_to_visits = (visits_after_sunset.iloc[0].obs_start_mjd - civil_sunset.mjd) \n",
    "if night_visits == 0:\n",
    "    sunset_to_visits = night_length\n",
    "    civilsunset_to_visits = night_length\n",
    "    onsky_time = {}\n",
    "\n",
    "dd = pd.DataFrame([night_length, time_queue_active, time_with_targets, \n",
    "                   sunset_to_queue, sunset_to_targets, sunset_to_visits, civilsunset_to_visits,\n",
    "                   time_in_breaks, time_overhead,], \n",
    "                  index=[\"Total night time\", \"Time ScriptQueue Active\", \"Time with Targets\",\n",
    "                         \"Sunset to Scriptqueue\", \"Sunset to Targets\", \"Sunset to Visits\", \"Civil Sunset to Visits\",\n",
    "                         \"Possible issue time\", \"Predicted Overheads\", \n",
    "                         ], \n",
    "                  columns=[day_obs])\n",
    "dd_obs = pd.DataFrame(onsky_time, index=[day_obs])\n",
    "# convert days (above) to hours\n",
    "dd = (dd * 24).T\n",
    "dd_obs = (dd_obs * 24)\n",
    "\n",
    "display(Markdown(\"Prototyping summary of time accounting, WIP\"))\n",
    "display(Markdown(\"Units: Hours. Sunset: -12 degree.\"))\n",
    "display(dd.round(2))\n",
    "if len(visits) > 0:\n",
    "    display(Markdown(\"Exposure time per science_program\"))\n",
    "    display(dd_obs.round(2))\n",
    "else:\n",
    "    display(Markdown(\"No consdb visits; not counting time per science_program\"))\n",
    "\n",
    "dd = dd.join(dd_obs)\n",
    "if dd[\"Time ScriptQueue Active\"].iloc[0] > 0:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 8))\n",
    "    fig.tight_layout(pad=17)\n",
    "    if len(visits) > 0:\n",
    "        cols = list(onsky_time.keys()) #+ [\"Possible issue time\", \"Overheads\", ]\n",
    "        plot_data = dd.loc[:, cols] / dd.iloc[0, 1]\n",
    "        plot_data.plot(kind='bar', stacked=True, ax=ax[0],\n",
    "                       ylim=[0, 1],\n",
    "                       legend=False, ylabel='Fraction of Time with Active ScriptQueue').legend(loc=(1.01, 0.2))\n",
    "        ax[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    cols = list(onsky_time.keys()) + [\"Time with Targets\", \"Time ScriptQueue Active\", \"Total night time\"]\n",
    "    plot_data = dd.loc[:, cols]\n",
    "    plot_data\n",
    "    plot_data.plot(kind='bar', stacked=False, ax=ax[-1],\n",
    "                   legend=False, ylabel='Hours').legend(loc=(1.01, 0.2))\n",
    "    ax[-1].grid(True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d6d6a6-55b3-4936-9b03-170335ebea1a",
   "metadata": {},
   "source": [
    "<a id=\"Summary_plot\"></a>\n",
    "\n",
    "## Summary Plot of FBS Targets, Observations, and ConsDB Visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89bf7e-ba97-4167-8f48-2999702d6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mjd_to_datetime(mjd, timezone=tz):\n",
    "    return Time(mjd, format='mjd', scale='utc').to_datetime(timezone=timezone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9cdb65-a38c-49f3-a86c-68427d475110",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1\n",
    "fig, ax = plt.subplots(figsize=(11, 7))\n",
    "ax_utc = ax.twiny()\n",
    "\n",
    "ax.set_title(f\"{telescope} DAYOBS {day_obs}\", pad=20)\n",
    "\n",
    "\n",
    "# Shade astronomical events\n",
    "ax.fill_between([mjd_to_datetime(night_events['sun_n12_setting']), \n",
    "                  mjd_to_datetime(night_events['sun_n18_setting'])],\n",
    "                 2.5, 0.0, color='lightgray', alpha=0.3)\n",
    "ax.fill_between([mjd_to_datetime(night_events['sunset']), \n",
    "                 mjd_to_datetime(night_events['sun_n12_setting'])], \n",
    "                  2.5, 0.0, color='gray', alpha=0.3)\n",
    "ax.fill_between([mjd_to_datetime(night_events['sun_n18_rising']), \n",
    "                 mjd_to_datetime(night_events['sun_n12_rising'])],\n",
    "                 2.5, 0.0, color='lightgray', alpha=0.3)\n",
    "ax.fill_between([mjd_to_datetime(night_events['sun_n12_rising']), \n",
    "                 mjd_to_datetime(night_events['sunrise'])],\n",
    "                 2.5, 0.0, color='gray', alpha=0.3)\n",
    "\n",
    "if not np.isnan(night_events['moonrise']):\n",
    "    ax.axvline(mjd_to_datetime(night_events['moonrise']), linestyle=':', color='blue', alpha=0.3)\n",
    "if not np.isnan(night_events['moonset']):\n",
    "    ax.axvline(mjd_to_datetime(night_events['moonset']), linestyle=':', color='red', alpha=0.3)\n",
    "\n",
    "plot_fbs = True\n",
    "\n",
    "plot_visits = True\n",
    "if len(visits) == 0:\n",
    "    plot_visits = False\n",
    "\n",
    "# Assign distinct target sets with different colors\n",
    "if len(jj) > 0:\n",
    "    notes = jj.note.unique()\n",
    "    note_colors = {}\n",
    "    for i, n in enumerate(notes):\n",
    "        note_colors[n] = cc.glasbey[i]\n",
    "elif len(visits) > 0:\n",
    "    notes = visits.note.unique()\n",
    "    note_colors = {}\n",
    "    for i, n in enumerate(notes):\n",
    "        note_colors[n] = cc.glasbey[i]\n",
    "else: # no completed observations, no visits, but targets\n",
    "    notes = targets.note.unique()\n",
    "    note_colors = {}\n",
    "    for i, n in enumerate(notes):\n",
    "        note_colors[n] = cc.glasbey[i]\n",
    "\n",
    "if plot_fbs and len(jj) > 0:\n",
    "    # Plot the successfully recorded 'observation' scripts\n",
    "    for note in jj.note.unique():\n",
    "        j = jj.query('note == @note')\n",
    "        ax.plot(mjd_to_datetime(j.obs_obsmjd), j.airmass, '.', \n",
    "                markersize=9,  color=note_colors[note], label=f\"Target+Obs {note}\")\n",
    "\n",
    "    # Shade breaks in observation events\n",
    "    if len(breaks) > 0:\n",
    "        for break_count, b in enumerate(breaks):\n",
    "            ax.fill_between(mjd_to_datetime(b), 2.5, 0.0, color='pink', alpha=0.3)\n",
    "            ax.text(mjd_to_datetime(b[0]), 0.97, f\"D-{break_count}\")\n",
    "\n",
    "    # Plot the observations that come after what looks like big overheads\n",
    "    #j = jj.iloc[overheads]\n",
    "    #plt.plot(j.obs_obsmjd, j.airmass, 'o', \n",
    "    #         markersize=10, color='k', markerfacecolor='none', label='Big Overheads')\n",
    "\n",
    "if plot_fbs and len(targets) > 0:\n",
    "\n",
    "    snapshot_times = Time(snapshots.index.values).to_datetime()\n",
    "    ax.plot(snapshot_times, np.ones(len(snapshots)) * 1.01, color='orange', \n",
    "            marker='|', markersize=13, linestyle='')\n",
    "    \n",
    "    # Plot the targets which did not get 'observations' recorded\n",
    "    incomplete_targets = targets.query('obs_id == -1')\n",
    "    for note in incomplete_targets.note.unique():\n",
    "        try:\n",
    "            color = note_colors[note]\n",
    "        except KeyError:\n",
    "            color = cc.glasbey[i]\n",
    "            i -= 1\n",
    "        t = incomplete_targets.query('note == @note')\n",
    "        ax.plot(mjd_to_datetime(t.target_obsmjd), \n",
    "                 t.airmass, '+', markersize=11,\n",
    "                 label=f'TargetOnly {note}', color=color, zorder=1)\n",
    "\n",
    "    # Shade incomplete target streaks\n",
    "    if len(breaks_targets) > 0:\n",
    "        for break_count, b in enumerate(breaks_targets):\n",
    "            ax.fill_between(mjd_to_datetime(b), 2.5, 0.0, color='lightblue', alpha=0.3)\n",
    "            ax.text(mjd_to_datetime(b[0]), 0.94, f\"T-{break_count}\")\n",
    "\n",
    "\n",
    "    # Shade time before first targets \n",
    "    # ax.fill_between([mjd_to_datetime(sunset.mjd), mjd_to_datetime(targets.target_mjd[0])],\n",
    "    #                2.5, 0.0, color='darkorange', alpha=0.1)\n",
    "\n",
    "\n",
    "i = len(cc.glasbey) - 1\n",
    "if plot_visits: \n",
    "    # Plot the visits recorded in the consdb \n",
    "    obs_visits = visits.query('img_type == \"OBJECT\"')\n",
    "    ac_visits = visits.query('img_type == \"ACQ\"')\n",
    "    focus_visits = visits.query('img_type == \"FOCUS\"')\n",
    "    cwfs_visits = visits.query('img_type == \"CWFS\"')\n",
    "    for note in visits.note.unique():\n",
    "        try:\n",
    "            color = note_colors[note]\n",
    "        except KeyError:\n",
    "            color = cc.glasbey[i]\n",
    "            i -= 1\n",
    "        # Label the first (in order) of these we find\n",
    "        label = f'ConsDb {note}'\n",
    "        v = obs_visits.query('note == @note')\n",
    "        if len(v) > 0:\n",
    "            ax.plot(mjd_to_datetime(v.obs_start_mjd), v.airmass, 'o', \n",
    "                     markersize=7, markerfacecolor='none', alpha=0.8,\n",
    "                     label=label, color=color, zorder=3)\n",
    "            label = None\n",
    "        v = cwfs_visits.query('note == @note')\n",
    "        if len(v) > 0:\n",
    "            ax.plot(mjd_to_datetime(v.obs_start_mjd), v.airmass, 'H', \n",
    "                     markersize=8, markerfacecolor='none', alpha=0.5,\n",
    "                     label=label, color=color, zorder=3)\n",
    "            label = None\n",
    "        v = ac_visits.query('note == @note')\n",
    "        if len(v) > 0:\n",
    "            ax.plot(mjd_to_datetime(v.obs_start_mjd), v.airmass, '*', \n",
    "                     markersize=9, markerfacecolor='none', alpha=0.5,\n",
    "                     label=label, color=color, zorder=3)\n",
    "            label = None\n",
    "        v = focus_visits.query('note == @note')\n",
    "        if len(v) > 0:\n",
    "            ax.plot(mjd_to_datetime(v.obs_start_mjd), v.airmass, 'D', \n",
    "                     markersize=5, markerfacecolor='none', alpha=0.5,\n",
    "                     label=label, color=color, zorder=3)\n",
    "            label = None\n",
    "\n",
    "ax.legend(loc=(1.01, 0.0), ncol=2)\n",
    "\n",
    "x0 = night_events['sunset']+30/60/24\n",
    "\n",
    "ax.set_xlim(mjd_to_datetime(night_events['sunset']+30/60/24), \n",
    "         mjd_to_datetime(night_events['sunrise']-30/60/24))\n",
    "ax_utc.set_xlim(mjd_to_datetime(night_events['sunset']+30/60/24, timezone=tz_utc), \n",
    "         mjd_to_datetime(night_events['sunrise']-30/60/24, timezone=tz_utc))\n",
    "\n",
    "ax.set_xlim(mjd_to_datetime(night_events['sunset']+30/60/24), \n",
    "         mjd_to_datetime(night_events['sunrise']-30/60/24))\n",
    "ax_utc.set_xlim(mjd_to_datetime(night_events['sunset']+30/60/24, timezone=tz_utc), \n",
    "         mjd_to_datetime(night_events['sunrise']-30/60/24, timezone=tz_utc))\n",
    "\n",
    "# Set ticks relevant sides\n",
    "ax.tick_params(axis=\"x\", bottom=True, top=False, labelbottom=True, labeltop=False)\n",
    "ax_utc.tick_params(axis=\"x\", bottom=False, top=True, labelbottom=False, labeltop=True)\n",
    "\n",
    "# Rotate and align bottom ticklabels\n",
    "plt.setp([tick.label1 for tick in ax.xaxis.get_major_ticks()], rotation=45,\n",
    "         ha=\"right\", va=\"center\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Rotate and align top ticklabels\n",
    "plt.setp([tick.label2 for tick in ax_utc.xaxis.get_major_ticks()], rotation=45,\n",
    "         ha=\"left\", va=\"center\",rotation_mode=\"anchor\")\n",
    "\n",
    "plt.grid(True, alpha=0.2)\n",
    "\n",
    "plt.ylim(2.3, 0.9)\n",
    "\n",
    "ax.set_ylabel(\"Airmass\", fontsize=\"large\")\n",
    "ax.set_xlabel(f\"Time ({tz})\", fontsize=\"large\")\n",
    "ax_utc.set_xlabel(\"Time (UTC)\", fontsize='large')\n",
    "_ = plt.ylabel(\"Airmass\", fontsize=\"large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bab81a-fea1-4173-acd7-003b24190b0a",
   "metadata": {},
   "source": [
    "Visits from the ConsDB, as well as `Targets` and `Observations` from the EFD records generated from the FeatureBasedScheduler. <br>\n",
    "The plot above stretches from sunset to sunrise, with civil, -12 degree and -18 degree sunset and sunrise indicated by the intensity of the gray shading. A blue (red) dashed line indicates the time of moonrise (moonset), if applicable. \n",
    "\n",
    "Blue shaded regions (if present) indicate periods of time with multiple sequential `Targets` that failed to find an expected `Observation` record in the EFD, and are numbered T-X. <br>\n",
    "Pink shaded regions (if present) indicate larger than expected overheads between sequential `Observation` events, and are numbered D-X. <br> \n",
    "Periods with both a delay of target and a series of sequential target failures are shaded both pink and blue, which results in lavender regions (but these are not double-counted above in possible issue time). <br>\n",
    "A orange shaded region (if present) on the left of the plot indicates the time from sunset until the time the first `Target` is sent from the FBS.\n",
    "\n",
    "`Targets` which have been linked with a corresponding `Observation` event (indicating the observing script completed successfully) are indicated by solid circles. `Targets` which were not able to be linked to an `Observation` are shown by crosses. \n",
    "\n",
    "`Visits` for `OBJECT` images are indicated by open circles, while `ACQ` `visits` are indicated by stars; `FOCUS` `visits` are shown with diamonds while `CWFS` `visits` are shown by hexagons.\n",
    "\n",
    "Both `Targets`, `Observations` and `Visits` are color-coded by their `note` values which are constructed in this notebook (using a combination of science_program and target_name). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2431ab-1541-455a-b883-53124def058d",
   "metadata": {},
   "source": [
    "<a id=\"Night_report\"></a>\n",
    "\n",
    "## Night Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a934777e-254c-43aa-92dd-20d5508fc07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def usdf_requests(API_ENDPOINT, auth, params):\n",
    "    # Try twice\n",
    "    response = requests.get(API_ENDPOINT, auth=auth, params=params)\n",
    "    if response.status_code != 200:\n",
    "        response = requests.get(API_ENDPOINT, auth=auth, params=params)\n",
    "    # Try dev as backup if still not working ..\n",
    "    if response.status_code != 200:\n",
    "        API_DEV = API_ENDPOINT.replace('usdf-rsp', 'usdf-rsp-dev')\n",
    "        # Try twice\n",
    "        response = requests.get(API_DEV, auth=auth, params=params)\n",
    "        if response.status_code != 200:\n",
    "            response = requests.get(API_DEV, auth=auth, params=params)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"{API_ENDPOINT} and {API_DEV} seem to be down.\")\n",
    "        messages = []\n",
    "    else:\n",
    "        messages = response.json()\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f7933-e65f-4251-9d55-f014730ae501",
   "metadata": {},
   "outputs": [],
   "source": [
    "if telescope.startswith(\"Aux\"):\n",
    "    tel_nr = \"AuxTel\"\n",
    "else:\n",
    "    tel_nr = \"Simonyi\"\n",
    "\n",
    "this_dayobs = day_obs.replace('-', '')\n",
    "next_dayobs = (Time(day_obs, format='iso') + TimeDelta(1, format='jd')).iso[0:10].replace('-', '')\n",
    "\n",
    "params = {\"telescopes\" : tel_nr,\n",
    "          \"min_day_obs\" : this_dayobs,\n",
    "          \"max_day_obs\" : next_dayobs,\n",
    "          \"is_valid\" : \"true\",\n",
    "         }\n",
    "\n",
    "token = get_access_token()\n",
    "auth = (\"user\", token)\n",
    "\n",
    "API_ENDPOINT = \"https://usdf-rsp.slac.stanford.edu/nightreport/reports\"\n",
    "\n",
    "logs = usdf_requests(API_ENDPOINT, auth=auth, params=params)\n",
    "    \n",
    "if len(logs) == 0:\n",
    "    print(\"No night report available.\")\n",
    "\n",
    "\n",
    "if len(logs) > 0:\n",
    "    for log in [logs[0]]:\n",
    "        display(Markdown(f\"Observing crew : {log['observers_crew']}\"))\n",
    "        night_plan_block = \"BLOCK\" + urllib.parse.urlparse(log['confluence_url']).fragment.split(\"BLOCK\")[-1]\n",
    "        if night_plan_block == \"BLOCK\":\n",
    "            night_plan_block = log['confluence_url']\n",
    "        url = log['confluence_url']\n",
    "        display(Markdown(f'Night plan : <a href=\"{url}\" target=\"_blank\" rel=\"noreferrer noopener\">{night_plan_block}</a>'))\n",
    "        display(Markdown(\"<strong>Summary</strong>\"))\n",
    "        display(Markdown(log['summary']))\n",
    "        display(Markdown(\"<strong>Status</strong>\"))\n",
    "        display(Markdown(log['telescope_status']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8fd9f9-8f6c-4f87-8c4f-46d119253d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
